{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86581b2",
   "metadata": {},
   "source": [
    "# Experiment 5  VKG Capability-Tier Evaluation\n",
    "\n",
    "Produces **Tables 1â€“6** for the paper. No monotonic enforcement  results arise naturally from cumulative feature design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff42340",
   "metadata": {},
   "source": [
    "# 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6d7ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda  |  Base: c:\\Users\\udipt\\Desktop\\MICCAI 26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os, warnings, time, gc\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from math import log2\n",
    "from collections import defaultdict\n",
    "from rdflib import Graph as RDFGraph, Namespace, RDF\n",
    "from rdflib.namespace import XSD\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RS = 42\n",
    "np.random.seed(RS); torch.manual_seed(RS)\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "KG_DIR  = os.path.join(BASE_DIR, 'kg')\n",
    "FIG_DIR = os.path.join(BASE_DIR, 'figures/200queries')\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "print(f'Device: {device}  |  Base: {BASE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a73f6f",
   "metadata": {},
   "source": [
    "# 2. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f4fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreas: 281 scans\n",
      "LiTS: 118 scans\n",
      "FLARE: 597 scans\n"
     ]
    }
   ],
   "source": [
    "pancreas_raw = pd.read_csv(os.path.join(DATA_DIR, 'nii', 'Pancreas_Tumor_Analysis.csv'))\n",
    "lits_raw = pd.read_csv(os.path.join(DATA_DIR, 'LiTS_newUpdate', 'LiTS_Liver_Tumor_Analysis.csv'))\n",
    "flare_raw = pd.read_csv(os.path.join(DATA_DIR, 'Flare23_newUpdate',\n",
    "                         'FLARE2023_tumor_organ_analysis_overlap_aware.csv'))\n",
    "\n",
    "def agg_pancreas(df):\n",
    "    a = df.groupby('CT Scan').agg({'Tumor Coverage (%)':['max','mean'],\n",
    "        'Tumor Volume':'sum','Pancreas Volume':'first','Distance (voxels)':'min'})\n",
    "    a.columns = ['max_cov','mean_cov','total_vol','organ_vol','min_dist']\n",
    "    a['tumor_count'] = 1; a['num_organs'] = 1\n",
    "    return a.reset_index()\n",
    "\n",
    "def agg_lits(df):\n",
    "    a = df.groupby('CT Scan').agg({'Tumor Coverage (%)':['max','mean'],\n",
    "        'Tumor Volume':'sum','Total Tumor Count':'first',\n",
    "        'Liver Volume':'first','Connected Organ Distance (voxels)':'min'})\n",
    "    a.columns = ['max_cov','mean_cov','total_vol','tumor_count','organ_vol','min_dist']\n",
    "    a['num_organs'] = 1\n",
    "    return a.reset_index()\n",
    "\n",
    "def agg_flare(df):\n",
    "    organ_vol_cols = ['Liver Volume','Right Kidney Volume','Spleen Volume',\n",
    "                      'Pancreas Volume','Left Kidney Volume']\n",
    "    df = df.copy()\n",
    "    df['_total_organ_vol'] = df[organ_vol_cols].fillna(0).sum(axis=1)\n",
    "    a = df.groupby('CT Scan').agg({'Tumor Coverage (%)':['max','mean'],\n",
    "        'Tumor Volume':'sum','Total Tumor Count':'first',\n",
    "        'Connected Organ Distance (voxels)':'min','_total_organ_vol':'first',\n",
    "        'Connected Organ': lambda x: x.nunique()})\n",
    "    a.columns = ['max_cov','mean_cov','total_vol','tumor_count',\n",
    "                 'min_dist','organ_vol','num_organs']\n",
    "    return a.reset_index()\n",
    "\n",
    "scan_dfs = {'Pancreas': agg_pancreas(pancreas_raw),\n",
    "            'LiTS': agg_lits(lits_raw), 'FLARE': agg_flare(flare_raw)}\n",
    "for n, d in scan_dfs.items(): print(f'{n}: {len(d)} scans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b726b1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreas: 281/281 matched -> CLIP (281, 512)\n",
      "LiTS: 118/118 matched -> CLIP (118, 512)\n",
      "FLARE: 422/597 matched -> CLIP (422, 512)\n"
     ]
    }
   ],
   "source": [
    "FEAT_PATHS = {\n",
    "    'Pancreas': os.path.join(DATA_DIR,'nii','viz','image_features.csv'),\n",
    "    'LiTS': os.path.join(DATA_DIR,'LiTS_newUpdate','3D_Visualizations','image_features.csv'),\n",
    "    'FLARE': os.path.join(DATA_DIR,'Flare23_newUpdate',\n",
    "             'Results_updated_overlap_kg_final','3D_Visualizations','image_features.csv')}\n",
    "ZS = ['tumorBurden','tumorShape','tumorPosition','tumorOrganRelation',\n",
    "      'organCount','structuralComplexity','tumorVisibility','spatialExtent']\n",
    "\n",
    "def load_clip(dname):\n",
    "    fd = pd.read_csv(FEAT_PATHS[dname]); sd = scan_dfs[dname]\n",
    "    suf = '_combined_visualization.png' if dname=='Pancreas' else '_3D_visualization.png'\n",
    "    fd['sid'] = fd['filename'].str.replace(suf, '', regex=False)\n",
    "    lk = {}\n",
    "    for _, r in fd.iterrows():\n",
    "        lk[r['sid']] = (np.fromstring(r['visualEmbedding'], sep=','),\n",
    "                         np.array([float(r[f'{z}_confidence']) for z in ZS]))\n",
    "    embs, zss, idx = [], [], []\n",
    "    for i, ct in enumerate(sd['CT Scan']):\n",
    "        if ct in lk:\n",
    "            e, z = lk[ct]; embs.append(e); zss.append(z); idx.append(i)\n",
    "    return np.array(embs), np.array(zss), idx\n",
    "\n",
    "clip_data = {}\n",
    "for dn in ['Pancreas','LiTS','FLARE']:\n",
    "    e, z, ix = load_clip(dn); clip_data[dn] = {'emb':e,'zs':z,'idx':ix}\n",
    "    print(f'{dn}: {len(ix)}/{len(scan_dfs[dn])} matched -> CLIP {e.shape}')\n",
    "\n",
    "for dn in scan_dfs:\n",
    "    scan_dfs[dn] = scan_dfs[dn].iloc[clip_data[dn]['idx']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32421be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading semantic model ...\n",
      "  -> S-PubMedBERT\n",
      "  Pancreas: text_emb (281, 768)\n",
      "  LiTS: text_emb (118, 768)\n",
      "  FLARE: text_emb (422, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "print('Loading semantic model ...')\n",
    "try:\n",
    "    sem_model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO', device='cpu')\n",
    "    SEM_NAME = 'S-PubMedBERT'\n",
    "except:\n",
    "    sem_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    SEM_NAME = 'MiniLM-L6'\n",
    "print(f'  -> {SEM_NAME}')\n",
    "\n",
    "def scan_to_text(r):\n",
    "    tc = int(r['tumor_count'])\n",
    "    p = [f'Abdominal CT scan with maximum tumor coverage {r[\"max_cov\"]:.2f} percent,',\n",
    "         f'total volume {int(r[\"total_vol\"])} voxels,',\n",
    "         f'minimum organ distance {r[\"min_dist\"]:.1f} voxels.',\n",
    "         f'{tc} tumor{\"s\" if tc>1 else \"\"}.']\n",
    "    if r.get('num_organs',1)>1: p.append(f'{int(r[\"num_organs\"])} organs involved.')\n",
    "    return ' '.join(p)\n",
    "\n",
    "text_embs = {}\n",
    "for dn in scan_dfs:\n",
    "    txts = [scan_to_text(r) for _,r in scan_dfs[dn].iterrows()]\n",
    "    text_embs[dn] = sem_model.encode(txts, show_progress_bar=False, batch_size=64)\n",
    "    print(f'  {dn}: text_emb {text_embs[dn].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a31b7",
   "metadata": {},
   "source": [
    "# 3. KG Loading & Topology Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53883d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreas: 281 scans in KG | +0 adjacency | +0 proximity edges\n",
      "LiTS: 118 scans in KG | +0 adjacency | +18728 proximity edges\n",
      "FLARE: 597 scans in KG | +10 adjacency | +12050 proximity edges\n"
     ]
    }
   ],
   "source": [
    "EX = Namespace('http://example.org/pancreas-kg#')\n",
    "KG_FILES = {\n",
    "    'LiTS': os.path.join(KG_DIR, 'kgLiTSv2.ttl'),\n",
    "    'Pancreas': os.path.join(KG_DIR, 'kg_pancreas.ttl'),\n",
    "    'FLARE': os.path.join(KG_DIR, 'kgFlare23v2.ttl')}\n",
    "\n",
    "ADJACENCY_PAIRS = [\n",
    "    ('Liver', 'Pancreas'), ('Liver', 'Right_Kidney'),\n",
    "    ('Pancreas', 'Spleen'), ('Left_Kidney', 'Spleen'),\n",
    "    ('Pancreas', 'Left_Kidney'), ('Pancreas', 'Right_Kidney'),\n",
    "]\n",
    "\n",
    "def _get_float(g, uri, prop, default=0.0):\n",
    "    vals = list(g.objects(uri, prop))\n",
    "    return float(vals[0]) if vals else default\n",
    "\n",
    "def load_kg_with_topology(kg_path):\n",
    "    g = RDFGraph(); g.parse(kg_path, format='turtle')\n",
    "    scan_uris = {}\n",
    "    for s, _, _ in g.triples((None, RDF.type, EX.CTScan)):\n",
    "        sid_lit = list(g.objects(s, EX.ctScanID))\n",
    "        if sid_lit:\n",
    "            scan_uris[str(sid_lit[0])] = s\n",
    "\n",
    "    for sid, s_uri in scan_uris.items():\n",
    "        tumors = list(g.objects(s_uri, EX.hasTumor))\n",
    "        scan_organs = {}\n",
    "        for t in tumors:\n",
    "            for o_uri in g.objects(t, EX.connectedToOrgan):\n",
    "                o_str = str(o_uri).split('#')[-1]\n",
    "                for known in ['Liver','Pancreas','Spleen','Right_Kidney','Left_Kidney']:\n",
    "                    if known.lower() in o_str.lower():\n",
    "                        scan_organs[known] = o_uri\n",
    "                        break\n",
    "        for o1_name, o2_name in ADJACENCY_PAIRS:\n",
    "            if o1_name in scan_organs and o2_name in scan_organs:\n",
    "                g.add((scan_organs[o1_name], EX.adjacentToOrgan, scan_organs[o2_name]))\n",
    "                g.add((scan_organs[o2_name], EX.adjacentToOrgan, scan_organs[o1_name]))\n",
    "        organ_tumors = defaultdict(list)\n",
    "        for t in tumors:\n",
    "            for o_uri in g.objects(t, EX.connectedToOrgan):\n",
    "                organ_tumors[o_uri].append(t)\n",
    "        for o_uri, tlist in organ_tumors.items():\n",
    "            for i in range(len(tlist)):\n",
    "                for j in range(i+1, len(tlist)):\n",
    "                    g.add((tlist[i], EX.proximateToTumor, tlist[j]))\n",
    "                    g.add((tlist[j], EX.proximateToTumor, tlist[i]))\n",
    "    return g, scan_uris\n",
    "\n",
    "kg_graphs = {}; scan_uri_maps = {}\n",
    "for dn in ['Pancreas','LiTS','FLARE']:\n",
    "    if os.path.exists(KG_FILES[dn]):\n",
    "        g, su = load_kg_with_topology(KG_FILES[dn])\n",
    "        kg_graphs[dn] = g; scan_uri_maps[dn] = su\n",
    "        n_adj = len(list(g.triples((None, EX.adjacentToOrgan, None))))\n",
    "        n_prox = len(list(g.triples((None, EX.proximateToTumor, None))))\n",
    "        print(f'{dn}: {len(su)} scans in KG | +{n_adj} adjacency | +{n_prox} proximity edges')\n",
    "    else:\n",
    "        kg_graphs[dn] = None; scan_uri_maps[dn] = {}\n",
    "        print(f'{dn}: KG not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4bd5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreas: 281 subgraphs pre-built\n",
      "LiTS: 118 subgraphs pre-built\n",
      "FLARE: 422 subgraphs pre-built\n",
      "GNN ready (CPU).\n"
     ]
    }
   ],
   "source": [
    "NODE_DIMS = {0: 3, 1: 1, 2: 512, 3: 6}\n",
    "MAX_DIM = 512\n",
    "CAT_MAP = {'morphology':0,'spatial':1,'structural':2,'global':3,'appearance':4}\n",
    "gnn_device = torch.device('cpu')\n",
    "\n",
    "class ScanSubgraphGNN(nn.Module):\n",
    "    def __init__(self, d_hid=64, d_out=32):\n",
    "        super().__init__()\n",
    "        self.projs = nn.ModuleDict({str(k): nn.Linear(v, 32) for k,v in NODE_DIMS.items()})\n",
    "        self.conv1 = SAGEConv(32, d_hid)\n",
    "        self.conv2 = SAGEConv(d_hid, d_out)\n",
    "\n",
    "    def forward(self, x, edge_index, node_type, batch_idx):\n",
    "        h = torch.zeros(x.size(0), 32, device=x.device)\n",
    "        for tid, dim in NODE_DIMS.items():\n",
    "            mask = (node_type == tid)\n",
    "            if mask.any():\n",
    "                h[mask] = self.projs[str(tid)](x[mask, :dim])\n",
    "        h = F.relu(self.conv1(h, edge_index))\n",
    "        h = F.dropout(h, 0.3, training=self.training)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        return global_mean_pool(h, batch_idx)\n",
    "\n",
    "def build_scan_data(scan_idx, dn, include_topology=True):\n",
    "    g = kg_graphs[dn]\n",
    "    if g is None: return None\n",
    "    ct = scan_dfs[dn].iloc[scan_idx]['CT Scan']\n",
    "    s_uri = scan_uri_maps[dn].get(ct)\n",
    "    if s_uri is None: return None\n",
    "    tumor_uris = list(g.objects(s_uri, EX.hasTumor))\n",
    "    if not tumor_uris: return None\n",
    "\n",
    "    node_feats = []; node_map = {}; edges = set()\n",
    "\n",
    "    def add_node(uri, tid, feats):\n",
    "        if uri in node_map: return node_map[uri]\n",
    "        idx = len(node_feats); node_map[uri] = idx\n",
    "        node_feats.append((tid, feats)); return idx\n",
    "\n",
    "    def add_edge(a, b):\n",
    "        edges.add((a, b)); edges.add((b, a))\n",
    "\n",
    "    for t_uri in tumor_uris:\n",
    "        vol = _get_float(g, t_uri, EX.tumorVolume, 0)\n",
    "        cov = _get_float(g, t_uri, EX.tumorCoveragePercent, 0)\n",
    "        dist = _get_float(g, t_uri, EX.distanceToConnectedOrgan, 0)\n",
    "        add_node(t_uri, 0, [vol, cov, dist])\n",
    "\n",
    "    for t_uri in tumor_uris:\n",
    "        for o_uri in g.objects(t_uri, EX.connectedToOrgan):\n",
    "            ovol = _get_float(g, o_uri, EX.organVolume, 0)\n",
    "            o_idx = add_node(o_uri, 1, [ovol])\n",
    "            add_edge(node_map[t_uri], o_idx)\n",
    "\n",
    "    image_uris = set()\n",
    "    for t_uri in tumor_uris:\n",
    "        for img_uri in g.objects(t_uri, EX.hasVisualization):\n",
    "            clip_emb = clip_data[dn]['emb'][scan_idx].tolist()\n",
    "            i_idx = add_node(img_uri, 2, clip_emb)\n",
    "            add_edge(node_map[t_uri], i_idx)\n",
    "            image_uris.add(img_uri)\n",
    "\n",
    "    for img_uri in image_uris:\n",
    "        for f_uri in g.objects(img_uri, EX.hasFeature):\n",
    "            conf = _get_float(g, f_uri, EX.confidenceScore, 0)\n",
    "            cat_vals = list(g.objects(f_uri, EX.featureCategory))\n",
    "            cat = str(cat_vals[0]) if cat_vals else ''\n",
    "            one_hot = [0.0]*5\n",
    "            ci = CAT_MAP.get(cat, 0); one_hot[ci] = 1.0\n",
    "            f_idx = add_node(f_uri, 3, [conf] + one_hot)\n",
    "            add_edge(node_map[img_uri], f_idx)\n",
    "\n",
    "    if include_topology:\n",
    "        for uri, idx in list(node_map.items()):\n",
    "            for adj_uri in g.objects(uri, EX.adjacentToOrgan):\n",
    "                if adj_uri in node_map: add_edge(idx, node_map[adj_uri])\n",
    "        for t_uri in tumor_uris:\n",
    "            for pt_uri in g.objects(t_uri, EX.proximateToTumor):\n",
    "                if pt_uri in node_map: add_edge(node_map[t_uri], node_map[pt_uri])\n",
    "\n",
    "    if not node_feats: return None\n",
    "    x_list, type_list = [], []\n",
    "    for tid, feats in node_feats:\n",
    "        padded = (feats + [0.0]*MAX_DIM)[:MAX_DIM]\n",
    "        x_list.append(padded); type_list.append(tid)\n",
    "    if not edges: edges = {(0, 0)}\n",
    "    src = [e[0] for e in edges]; dst = [e[1] for e in edges]\n",
    "    return Data(x=torch.tensor(x_list, dtype=torch.float32),\n",
    "                edge_index=torch.tensor([src, dst], dtype=torch.long),\n",
    "                node_type=torch.tensor(type_list, dtype=torch.long))\n",
    "\n",
    "DUMMY = Data(x=torch.zeros(1, MAX_DIM), edge_index=torch.tensor([[0],[0]], dtype=torch.long),\n",
    "             node_type=torch.tensor([0], dtype=torch.long))\n",
    "\n",
    "def train_gnn_fold(train_indices, all_data_list, tab_train, k=5, epochs=200, lr=0.01):\n",
    "    train_data = [all_data_list[i] for i in train_indices]\n",
    "    n_train = len(train_data)\n",
    "    feat_sc = StandardScaler().fit_transform(tab_train)\n",
    "    kk = min(k, n_train - 1)\n",
    "    knn = kneighbors_graph(feat_sc, kk, mode='connectivity', include_self=False)\n",
    "    adj = np.maximum(knn.toarray(), knn.toarray().T)\n",
    "    pos_r, pos_c = np.where(adj > 0)\n",
    "\n",
    "    batch_train = Batch.from_data_list(train_data)\n",
    "    model = ScanSubgraphGNN(64, 32)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    pr = torch.tensor(pos_r, dtype=torch.long); pc = torch.tensor(pos_c, dtype=torch.long)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        embs = model(batch_train.x, batch_train.edge_index, batch_train.node_type, batch_train.batch)\n",
    "        pos_score = (embs[pr] * embs[pc]).sum(1)\n",
    "        neg_idx = torch.randint(0, n_train, (pr.size(0),))\n",
    "        neg_score = (embs[pr] * embs[neg_idx]).sum(1)\n",
    "        loss = -F.logsigmoid(pos_score).mean() - F.logsigmoid(-neg_score).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    batch_all = Batch.from_data_list(all_data_list)\n",
    "    with torch.no_grad():\n",
    "        result = model(batch_all.x, batch_all.edge_index, batch_all.node_type, batch_all.batch).numpy()\n",
    "    del batch_train, batch_all, model, opt, pr, pc; gc.collect()\n",
    "    return result\n",
    "\n",
    "scan_graphs = {}; scan_graphs_no_topo = {}\n",
    "for dn in ['Pancreas','LiTS','FLARE']:\n",
    "    n = len(scan_dfs[dn])\n",
    "    scan_graphs[dn] = [build_scan_data(i, dn, True) or DUMMY for i in range(n)]\n",
    "    scan_graphs_no_topo[dn] = [build_scan_data(i, dn, False) or DUMMY for i in range(n)]\n",
    "    print(f'{dn}: {n} subgraphs pre-built')\n",
    "print('GNN ready (CPU).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36e915",
   "metadata": {},
   "source": [
    "# 3b. Standard Terminology Enrichment (SNOMED CT, MeSH, LOINC, ICD-11)\n",
    "\n",
    "Query live terminology APIs to map KG concepts to standard medical codes.\n",
    "Annotate the schema and instance KGs with `rdfs:seeAlso` links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4169b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying standard terminology APIs ...\n",
      "\n",
      "  CTScan                       -> SNOMED:169064005, MeSH:T047116\n",
      "  Tumor                        -> SNOMED:108369006, MeSH:T190624\n",
      "  Organ                        -> SNOMED:116194001, MeSH:T001136313\n",
      "  Image                        -> SNOMED:708175003, MeSH:T011956\n",
      "  ImageFeature                 -> SNOMED:199737005\n",
      "  Patient                      -> SNOMED:116154003, MeSH:T047725\n",
      "  Liver                        -> SNOMED:303410000\n",
      "  Pancreas                     -> SNOMED:2504000\n",
      "  Spleen                       -> SNOMED:35819009\n",
      "  Right_Kidney                 -> SNOMED:9846003\n",
      "  Left_Kidney                  -> SNOMED:18639004\n",
      "  tumorVolume                  -> SNOMED:258261001, MeSH:T563247\n",
      "  tumorCoveragePercent         -> no matches\n",
      "  organVolume                  -> SNOMED:786041005, MeSH:T629614\n",
      "  distanceToConnectedOrgan     -> no matches\n",
      "  confidenceScore              -> SNOMED:443125007\n",
      "  totalTumorCount              -> no matches\n",
      "\n",
      "API hit counts: {'SNOMED CT': 14, 'MeSH': 7, 'LOINC': 0, 'ICD-11': 0}\n",
      "LOINC credentials: not set (LOINC_UID/LOINC_PWD)\n",
      "ICD-11 credentials: not set (ICD_CLIENT_TOKEN)\n",
      "\n",
      "--- Standard Terminology Mapping ---\n",
      "                 Concept     Type                  Search Term                                            SNOMED CT                                             MeSH LOINC ICD-11\n",
      "                  CTScan    class     computed tomography scan          169064005 (Computed tomography scan normal)     T047116 (Computed Tomography Scanner, X-Ray)     -      -\n",
      "                   Tumor    class                     neoplasm                                 108369006 (Neoplasm) T190624 (90K neoplasm-associated antigen, human)     -      -\n",
      "                   Organ    class              organ structure                                116194001 (Organelle)  T001136313 (Mitochondrial Organizing Structure)     -      -\n",
      "                   Image    class           diagnostic imaging               708175003 (Diagnostic imaging service)                     T011956 (Diagnostic Imaging)     -      -\n",
      "            ImageFeature    class         radiological finding 199737005 (Abnormal radiological finding on antenat)                                                -     -      -\n",
      "                 Patient    class                      patient                                  116154003 (Patient)          T047725 (Analgesia, Patient-Controlled)     -      -\n",
      "                   Liver    organ              liver structure              303410000 (Vascular structure of liver)                                                -     -      -\n",
      "                Pancreas    organ         pancreatic structure             2504000 (Structure of pancreatic plexus)                                                -     -      -\n",
      "                  Spleen    organ            splenic structure                 35819009 (Structure of splenic vein)                                                -     -      -\n",
      "            Right_Kidney    organ                 right kidney                     9846003 (Right kidney structure)                                                -     -      -\n",
      "             Left_Kidney    organ                  left kidney                               18639004 (Left kidney)                                                -     -      -\n",
      "             tumorVolume property                 tumor volume                            258261001 (Tumour volume)                           T563247 (Tumor Volume)     -      -\n",
      "    tumorCoveragePercent property tumor involvement percentage                                                    -                                                -     -      -\n",
      "             organVolume property                 organ volume 786041005 (Congenital systemic arteriovenous fistul)                           T629614 (Organ Volume)     -      -\n",
      "distanceToConnectedOrgan property            distance to organ                                                    -                                                -     -      -\n",
      "         confidenceScore property             confidence score 443125007 (ABC (activities-specific balance confide)                                                -     -      -\n",
      "         totalTumorCount property                  tumor count                                                    -                                                -     -      -\n",
      "\n",
      "Added 21 standard terminology annotations to schema\n",
      "Saved enriched schema: c:\\Users\\udipt\\Desktop\\MICCAI 26\\kg\\schema_unifiedv2_enriched.owl\n",
      "Added 2582 annotations across instance KGs\n",
      "\n",
      "Terminology enrichment complete.\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "from rdflib import URIRef, Literal\n",
    "from rdflib.namespace import RDFS, OWL, SKOS\n",
    "\n",
    "# ===== API Functions (adapted from terminologies.txt) =====\n",
    "\n",
    "def get_snomed_ct_code(variable, count=5):\n",
    "    \"\"\"Query SNOMED CT via Snowstorm FHIR (free, no auth).\"\"\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://snowstorm-training.snomedtools.org/fhir/ValueSet/$expand\",\n",
    "            params={'url': 'http://snomed.info/sct?fhir_vs', 'filter': variable, 'count': count},\n",
    "            timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return [{'system': 'SNOMED CT',\n",
    "                     'code': item['code'],\n",
    "                     'url': f\"http://snomed.info/sct/{item['code']}\",\n",
    "                     'display': item['display']}\n",
    "                    for item in r.json().get('expansion', {}).get('contains', [])]\n",
    "    except Exception as e:\n",
    "        print(f'  SNOMED error for \"{variable}\": {e}')\n",
    "    return []\n",
    "\n",
    "def get_mesh_code(query, count=5):\n",
    "    \"\"\"Query MeSH via NLM lookup (free, no auth).\"\"\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://id.nlm.nih.gov/mesh/lookup/term\",\n",
    "            params={'label': query, 'match': 'contains', 'limit': count},\n",
    "            timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return [{'system': 'MeSH',\n",
    "                     'code': item['resource'].split('/')[-1],\n",
    "                     'url': item['resource'],\n",
    "                     'display': item['label']}\n",
    "                    for item in r.json()]\n",
    "    except Exception as e:\n",
    "        print(f'  MeSH error for \"{query}\": {e}')\n",
    "    return []\n",
    "\n",
    "def get_loinc_code(query, count=5):\n",
    "    \"\"\"Query LOINC via Regenstrief (needs LOINC_UID/LOINC_PWD env vars).\"\"\"\n",
    "    uid = os.getenv('LOINC_UID')\n",
    "    pwd = os.getenv('LOINC_PWD')\n",
    "    if not uid or not pwd:\n",
    "        return []\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://loinc.regenstrief.org/searchapi/loincs\",\n",
    "            params={'query': query, 'rows': count},\n",
    "            auth=(uid, pwd), timeout=30)\n",
    "        if r.status_code == 200 and 'Results' in r.json():\n",
    "            return [{'system': 'LOINC',\n",
    "                     'code': item['LOINC_NUM'],\n",
    "                     'url': f\"https://loinc.org/{item['LOINC_NUM']}\",\n",
    "                     'display': item['LONG_COMMON_NAME']}\n",
    "                    for item in r.json()['Results']]\n",
    "    except Exception as e:\n",
    "        print(f'  LOINC error for \"{query}\": {e}')\n",
    "    return []\n",
    "\n",
    "def get_icd_code(variable, count=5):\n",
    "    \"\"\"Query ICD-11 via WHO API (needs ICD_CLIENT_TOKEN env var).\"\"\"\n",
    "    client_cred = os.getenv('ICD_CLIENT_TOKEN')\n",
    "    if not client_cred:\n",
    "        return []\n",
    "    try:\n",
    "        # Get bearer token\n",
    "        tok_r = requests.post(\n",
    "            \"https://icdaccessmanagement.who.int/connect/token\",\n",
    "            headers={'Authorization': f'Basic {client_cred}',\n",
    "                     'Content-Type': 'application/x-www-form-urlencoded'},\n",
    "            data={'grant_type': 'client_credentials'}, timeout=30)\n",
    "        if tok_r.status_code != 200:\n",
    "            return []\n",
    "        token = tok_r.json().get('access_token')\n",
    "        if not token:\n",
    "            return []\n",
    "        r = requests.get(\n",
    "            \"https://id.who.int/icd/entity/search\",\n",
    "            params={'q': variable, 'useFlexisearch': False, 'flatResults': True},\n",
    "            headers={'accept': 'application/json', 'API-Version': 'v2',\n",
    "                     'Accept-Language': 'en', 'Authorization': f'Bearer {token}'},\n",
    "            timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return [{'system': 'ICD-11',\n",
    "                     'code': ent.get('theCode', ent.get('id', '')),\n",
    "                     'url': ent.get('id', ''),\n",
    "                     'display': re.sub(r'</?em.*?>', '', ent.get('title', ''))}\n",
    "                    for ent in r.json().get('destinationEntities', [])[:count]]\n",
    "    except Exception as e:\n",
    "        print(f'  ICD error for \"{variable}\": {e}')\n",
    "    return []\n",
    "\n",
    "def pick_best(results, query_lower):\n",
    "    \"\"\"Simple heuristic: prefer exact-ish matches, then first result.\"\"\"\n",
    "    if not results:\n",
    "        return None\n",
    "    query_words = set(query_lower.split())\n",
    "    for r in results:\n",
    "        disp_lower = r['display'].lower()\n",
    "        if query_lower in disp_lower:\n",
    "            return r\n",
    "        if query_words.issubset(set(disp_lower.split())):\n",
    "            return r\n",
    "    return results[0]\n",
    "\n",
    "# ===== Concept-to-search-term mapping =====\n",
    "\n",
    "CONCEPT_MAP = {\n",
    "    # OWL Classes\n",
    "    'CTScan':       {'search': 'computed tomography scan', 'type': 'class'},\n",
    "    'Tumor':        {'search': 'neoplasm',                 'type': 'class'},\n",
    "    'Organ':        {'search': 'organ structure',          'type': 'class'},\n",
    "    'Image':        {'search': 'diagnostic imaging',       'type': 'class'},\n",
    "    'ImageFeature': {'search': 'radiological finding',     'type': 'class'},\n",
    "    'Patient':      {'search': 'patient',                  'type': 'class'},\n",
    "    # Specific organ instances\n",
    "    'Liver':        {'search': 'liver structure',          'type': 'organ'},\n",
    "    'Pancreas':     {'search': 'pancreatic structure',     'type': 'organ'},\n",
    "    'Spleen':       {'search': 'splenic structure',        'type': 'organ'},\n",
    "    'Right_Kidney': {'search': 'right kidney',             'type': 'organ'},\n",
    "    'Left_Kidney':  {'search': 'left kidney',              'type': 'organ'},\n",
    "    # Properties / measurements\n",
    "    'tumorVolume':              {'search': 'tumor volume',                  'type': 'property'},\n",
    "    'tumorCoveragePercent':     {'search': 'tumor involvement percentage',  'type': 'property'},\n",
    "    'organVolume':              {'search': 'organ volume',                  'type': 'property'},\n",
    "    'distanceToConnectedOrgan': {'search': 'distance to organ',             'type': 'property'},\n",
    "    'confidenceScore':          {'search': 'confidence score',              'type': 'property'},\n",
    "    'totalTumorCount':          {'search': 'tumor count',                   'type': 'property'},\n",
    "}\n",
    "\n",
    "# ===== Query all APIs =====\n",
    "\n",
    "print('Querying standard terminology APIs ...\\n')\n",
    "terminology_results = {}\n",
    "api_stats = {'SNOMED CT': 0, 'MeSH': 0, 'LOINC': 0, 'ICD-11': 0}\n",
    "\n",
    "for concept, info in CONCEPT_MAP.items():\n",
    "    query = info['search']\n",
    "    snomed = get_snomed_ct_code(query)\n",
    "    mesh = get_mesh_code(query)\n",
    "    loinc = get_loinc_code(query)\n",
    "    icd = get_icd_code(query)\n",
    "\n",
    "    best_snomed = pick_best(snomed, query.lower())\n",
    "    best_mesh = pick_best(mesh, query.lower())\n",
    "    best_loinc = pick_best(loinc, query.lower())\n",
    "    best_icd = pick_best(icd, query.lower())\n",
    "\n",
    "    terminology_results[concept] = {\n",
    "        'search_term': query,\n",
    "        'type': info['type'],\n",
    "        'snomed': best_snomed,\n",
    "        'mesh': best_mesh,\n",
    "        'loinc': best_loinc,\n",
    "        'icd': best_icd,\n",
    "        'all_snomed': snomed,\n",
    "        'all_mesh': mesh,\n",
    "    }\n",
    "    if best_snomed: api_stats['SNOMED CT'] += 1\n",
    "    if best_mesh: api_stats['MeSH'] += 1\n",
    "    if best_loinc: api_stats['LOINC'] += 1\n",
    "    if best_icd: api_stats['ICD-11'] += 1\n",
    "\n",
    "    matches = []\n",
    "    if best_snomed: matches.append(f'SNOMED:{best_snomed[\"code\"]}')\n",
    "    if best_mesh: matches.append(f'MeSH:{best_mesh[\"code\"]}')\n",
    "    if best_loinc: matches.append(f'LOINC:{best_loinc[\"code\"]}')\n",
    "    if best_icd: matches.append(f'ICD:{best_icd[\"code\"]}')\n",
    "    print(f'  {concept:<28s} -> {\", \".join(matches) if matches else \"no matches\"}')\n",
    "\n",
    "print(f'\\nAPI hit counts: {api_stats}')\n",
    "loinc_avail = api_stats['LOINC'] > 0\n",
    "icd_avail = api_stats['ICD-11'] > 0\n",
    "print(f'LOINC credentials: {\"available\" if loinc_avail else \"not set (LOINC_UID/LOINC_PWD)\"}')\n",
    "print(f'ICD-11 credentials: {\"available\" if icd_avail else \"not set (ICD_CLIENT_TOKEN)\"}')\n",
    "\n",
    "# ===== Build summary DataFrame =====\n",
    "\n",
    "summary_rows = []\n",
    "for concept, res in terminology_results.items():\n",
    "    row = {'Concept': concept, 'Type': res['type'], 'Search Term': res['search_term']}\n",
    "    for sys_key, sys_name in [('snomed','SNOMED CT'), ('mesh','MeSH'), ('loinc','LOINC'), ('icd','ICD-11')]:\n",
    "        best = res[sys_key]\n",
    "        if best:\n",
    "            row[sys_name] = f'{best[\"code\"]} ({best[\"display\"][:40]})'\n",
    "        else:\n",
    "            row[sys_name] = '-'\n",
    "    summary_rows.append(row)\n",
    "terminology_df = pd.DataFrame(summary_rows)\n",
    "print('\\n--- Standard Terminology Mapping ---')\n",
    "print(terminology_df.to_string(index=False))\n",
    "\n",
    "# ===== Annotate KG schema with rdfs:seeAlso =====\n",
    "\n",
    "SKOS_NS = Namespace('http://www.w3.org/2004/02/skos/core#')\n",
    "\n",
    "schema_g = RDFGraph()\n",
    "schema_path = os.path.join(KG_DIR, 'schema_unifiedv2.owl')\n",
    "schema_g.parse(schema_path, format='xml')\n",
    "schema_g.bind('skos', SKOS_NS)\n",
    "schema_g.bind('ex', EX)\n",
    "\n",
    "annotations_added = 0\n",
    "for concept, res in terminology_results.items():\n",
    "    concept_uri = EX[concept]\n",
    "    for sys_key in ['snomed', 'mesh', 'loinc', 'icd']:\n",
    "        best = res[sys_key]\n",
    "        if best and best.get('url'):\n",
    "            schema_g.add((concept_uri, RDFS.seeAlso, URIRef(best['url'])))\n",
    "            schema_g.add((concept_uri, SKOS_NS.exactMatch, URIRef(best['url'])))\n",
    "            annotations_added += 1\n",
    "\n",
    "enriched_path = os.path.join(KG_DIR, 'schema_unifiedv2_enriched.owl')\n",
    "schema_g.serialize(destination=enriched_path, format='xml')\n",
    "print(f'\\nAdded {annotations_added} standard terminology annotations to schema')\n",
    "print(f'Saved enriched schema: {enriched_path}')\n",
    "\n",
    "# ===== Annotate instance KGs with organ-level seeAlso =====\n",
    "\n",
    "organ_concepts = {c: r for c, r in terminology_results.items() if r['type'] == 'organ'}\n",
    "instance_annotations = 0\n",
    "for dn in ['Pancreas', 'LiTS', 'FLARE']:\n",
    "    g = kg_graphs.get(dn)\n",
    "    if g is None:\n",
    "        continue\n",
    "    g.bind('skos', SKOS_NS)\n",
    "    # Find organ instances and add standard codes\n",
    "    for s_uri in scan_uri_maps[dn].values():\n",
    "        for t_uri in g.objects(s_uri, EX.hasTumor):\n",
    "            for o_uri in g.objects(t_uri, EX.connectedToOrgan):\n",
    "                o_str = str(o_uri).split('#')[-1]\n",
    "                for organ_name, res in organ_concepts.items():\n",
    "                    if organ_name.lower().replace('_', '') in o_str.lower().replace('_', ''):\n",
    "                        for sys_key in ['snomed', 'mesh']:\n",
    "                            best = res[sys_key]\n",
    "                            if best and best.get('url'):\n",
    "                                g.add((o_uri, RDFS.seeAlso, URIRef(best['url'])))\n",
    "                                instance_annotations += 1\n",
    "                        break\n",
    "\n",
    "print(f'Added {instance_annotations} annotations across instance KGs')\n",
    "print('\\nTerminology enrichment complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e197fe5",
   "metadata": {},
   "source": [
    "# 4. VKG Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e75a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreas: VKG features (281, 20), non-zero: [281 281 281 281 281 281 281 281 281   0   0 281 281   0 281 281   0 281\n",
      " 281 281]\n",
      "LiTS: VKG features (118, 20), non-zero: [118 118 118 118 118 118 118 118 118   0  83 118 118  83 118 118  83 118\n",
      " 118 118]\n",
      "FLARE: VKG features (422, 20), non-zero: [422 422 422 422 422 422 422 422 422 422  69 422 422  69 422 422 422 422\n",
      " 422 422]\n"
     ]
    }
   ],
   "source": [
    "def extract_vkg_features(g, scan_df, scan_uris):\n",
    "    \"\"\"Extract 20-dim reasoning features from topology-augmented KG.\"\"\"\n",
    "    CAT_NAMES = ['morphology', 'spatial', 'structural', 'global', 'appearance']\n",
    "    feats = []\n",
    "    for _, row in scan_df.iterrows():\n",
    "        ct = row['CT Scan']\n",
    "        f = np.zeros(20)\n",
    "        s_uri = scan_uris.get(ct)\n",
    "        if s_uri is None: feats.append(f); continue\n",
    "        tumors = list(g.objects(s_uri, EX.hasTumor))\n",
    "        n_tumors = len(tumors)\n",
    "        if n_tumors == 0: feats.append(f); continue\n",
    "\n",
    "        n_complete = 0; all_confs = []; cat_confs = {c: [] for c in CAT_NAMES}\n",
    "        unique_feat_values = set(); unique_organs = set(); organ_uris = set()\n",
    "        organ_tumor_counts = defaultdict(int); tumor_degrees = []; organ_vols = {}\n",
    "        tumor_avg_confs = {}; max_coverage = 0.0\n",
    "\n",
    "        for t_uri in tumors:\n",
    "            orgs = list(g.objects(t_uri, EX.connectedToOrgan))\n",
    "            imgs = list(g.objects(t_uri, EX.hasVisualization))\n",
    "            t_degree = len(orgs) + len(imgs)\n",
    "            cov_vals = list(g.objects(t_uri, EX.tumorCoveragePercent))\n",
    "            if cov_vals: max_coverage = max(max_coverage, float(cov_vals[0]))\n",
    "            for o in orgs:\n",
    "                unique_organs.add(str(o)); organ_uris.add(o)\n",
    "                organ_tumor_counts[o] += 1\n",
    "                if o not in organ_vols:\n",
    "                    ov = list(g.objects(o, EX.organVolume))\n",
    "                    organ_vols[o] = float(ov[0]) if ov else 0.0\n",
    "            t_confs = []\n",
    "            if imgs:\n",
    "                img_feats_list = list(g.objects(imgs[0], EX.hasFeature))\n",
    "                t_degree += len(img_feats_list)\n",
    "                if img_feats_list and orgs: n_complete += 1\n",
    "                for ff in img_feats_list:\n",
    "                    cs = list(g.objects(ff, EX.confidenceScore))\n",
    "                    vs = list(g.objects(ff, EX.featureValue))\n",
    "                    cats = list(g.objects(ff, EX.featureCategory))\n",
    "                    if cs:\n",
    "                        c_val = float(cs[0]); all_confs.append(c_val); t_confs.append(c_val)\n",
    "                        cat_str = str(cats[0]) if cats else ''\n",
    "                        for cn in CAT_NAMES:\n",
    "                            if cn in cat_str.lower(): cat_confs[cn].append(c_val); break\n",
    "                    if vs: unique_feat_values.add(str(vs[0]))\n",
    "            tumor_degrees.append(t_degree)\n",
    "            tumor_avg_confs[t_uri] = np.mean(t_confs) if t_confs else 0.0\n",
    "\n",
    "        n_organs = len(unique_organs)\n",
    "        mean_conf = np.mean(all_confs) if all_confs else 0.0\n",
    "        f[0] = n_complete / max(n_tumors, 1)\n",
    "        f[1] = mean_conf\n",
    "        f[2] = np.std(all_confs) if len(all_confs) > 1 else 0.0\n",
    "        f[3] = len(unique_feat_values) / 8.0\n",
    "        for ci, cn in enumerate(CAT_NAMES):\n",
    "            f[4 + ci] = np.mean(cat_confs[cn]) if cat_confs[cn] else 0.0\n",
    "\n",
    "        adj_count = 0\n",
    "        for o_uri in organ_uris:\n",
    "            adj_count += len(list(g.objects(o_uri, EX.adjacentToOrgan)))\n",
    "        adj_density = adj_count / max(n_organs * (n_organs - 1), 1)\n",
    "        f[9] = adj_density\n",
    "        prox_count = 0\n",
    "        for t in tumors:\n",
    "            prox_count += len(list(g.objects(t, EX.proximateToTumor)))\n",
    "        prox_density = (prox_count // 2) / max(n_tumors * (n_tumors - 1), 1)\n",
    "        f[10] = prox_density\n",
    "        max_hops = 1 if organ_uris else 0\n",
    "        for o_uri in organ_uris:\n",
    "            if list(g.objects(o_uri, EX.adjacentToOrgan)): max_hops = max(max_hops, 2)\n",
    "        if n_complete > 0: max_hops = max(max_hops, 3)\n",
    "        f[11] = max_hops\n",
    "        f[12] = np.mean(tumor_degrees) if tumor_degrees else 0.0\n",
    "        if n_organs > 0:\n",
    "            f[13] = sum(1 for c in organ_tumor_counts.values() if c >= 2) / n_organs\n",
    "        total_weighted = 0.0; total_organ_vol = 0.0\n",
    "        for o_uri in organ_uris:\n",
    "            ov = organ_vols.get(o_uri, 0.0)\n",
    "            connected_tumors = [t for t in tumors if o_uri in list(g.objects(t, EX.connectedToOrgan))]\n",
    "            avg_t_conf = np.mean([tumor_avg_confs.get(t, 0.0) for t in connected_tumors]) if connected_tumors else 0.0\n",
    "            total_weighted += ov * avg_t_conf; total_organ_vol += ov\n",
    "        f[14] = total_weighted / max(total_organ_vol, 1.0)\n",
    "        f[15] = f[0] * mean_conf\n",
    "        f[16] = mean_conf * (adj_density + prox_density)\n",
    "        f[17] = mean_conf * (max_coverage / 100.0)\n",
    "        f[18] = len(all_confs)\n",
    "        f[19] = n_organs\n",
    "        feats.append(f)\n",
    "    return np.array(feats)\n",
    "\n",
    "vkg_feats = {}\n",
    "for dn in ['Pancreas','LiTS','FLARE']:\n",
    "    if kg_graphs[dn] is not None:\n",
    "        vkg_feats[dn] = extract_vkg_features(kg_graphs[dn], scan_dfs[dn], scan_uri_maps[dn])\n",
    "        print(f'{dn}: VKG features {vkg_feats[dn].shape}, non-zero: {(vkg_feats[dn] != 0).sum(axis=0)}')\n",
    "    else:\n",
    "        vkg_feats[dn] = np.zeros((len(scan_dfs[dn]), 20))\n",
    "        print(f'{dn}: KG not found, using zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877766f1",
   "metadata": {},
   "source": [
    "# 5. Dataset Assembly & Tier Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99649e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancreas:  T1=7  T2=39  T3=79  T4=111  T5=131\n",
      "LiTS:  T1=7  T2=39  T3=79  T4=111  T5=131\n",
      "FLARE:  T1=7  T2=39  T3=79  T4=111  T5=131\n",
      "\n",
      "PCA: 768->32 (text), 512->32 (CLIP)\n",
      "Tier assembly deferred to per-fold evaluation (no leakage).\n"
     ]
    }
   ],
   "source": [
    "TAB_COLS = ['max_cov','mean_cov','total_vol','organ_vol','min_dist','tumor_count','num_organs']\n",
    "N_PCA = 32\n",
    "\n",
    "datasets = {}\n",
    "for dn in ['Pancreas','LiTS','FLARE']:\n",
    "    df = scan_dfs[dn]\n",
    "    tab  = df[TAB_COLS].values.astype(float)\n",
    "    temb_raw = text_embs[dn]\n",
    "    cemb_raw = clip_data[dn]['emb']\n",
    "    czs  = clip_data[dn]['zs']\n",
    "    vkgf = vkg_feats[dn]\n",
    "    datasets[dn] = {\n",
    "        'scan_df': df, 'tab': tab, 'clip_zs': czs, 'vkg_feats': vkgf,\n",
    "        'text_emb_raw': temb_raw, 'clip_emb_raw': cemb_raw,\n",
    "    }\n",
    "    dims = {'T1': 7, 'T2': 7+N_PCA, 'T3': 7+N_PCA+N_PCA+8,\n",
    "            'T4': 7+N_PCA+N_PCA+8+32, 'T5': 7+N_PCA+N_PCA+8+32+20}\n",
    "    print(f'{dn}:  ' + '  '.join(f'{k}={v}' for k,v in dims.items()))\n",
    "\n",
    "def build_tiers_fold(tab, temb_pca, cemb_pca, czs, gemb, vkgf):\n",
    "    return {\n",
    "        'T1': tab,\n",
    "        'T2': np.hstack([tab, temb_pca]),\n",
    "        'T3': np.hstack([tab, temb_pca, cemb_pca, czs]),\n",
    "        'T4': np.hstack([tab, temb_pca, cemb_pca, czs, gemb]),\n",
    "        'T5': np.hstack([tab, temb_pca, cemb_pca, czs, gemb, vkgf]),\n",
    "    }\n",
    "\n",
    "print(f'\\nPCA: 768->{N_PCA} (text), 512->{N_PCA} (CLIP)')\n",
    "print('Tier assembly deferred to per-fold evaluation (no leakage).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a3305",
   "metadata": {},
   "source": [
    "# 6. Evaluation Infrastructure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c79483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 structured queries (paper Sec 4.4)\n",
      "  Cardinality: {2: 100, 1: 50, 3: 50}\n",
      "  Min tier needed: {'T1': 54, 'T3': 23, 'T4': 20, 'T5': 103}\n",
      "Tier-masked XGBoost: noise=0.2\n",
      "  T1: 7 visible / 131 total\n",
      "  T2: 39 visible / 131 total\n",
      "  T3: 79 visible / 131 total\n",
      "  T4: 111 visible / 131 total\n",
      "  T5: 131 visible / 131 total\n"
     ]
    }
   ],
   "source": [
    "def normalize_with_train(x, train_idx):\n",
    "    mn = x[train_idx].min(); mx = x[train_idx].max()\n",
    "    return np.clip((x - mn) / (mx - mn + 1e-8), 0, 1)\n",
    "\n",
    "# --- Predicate functions ---\n",
    "def pred_high_volume(tab, czs, vkgf, tr):   return normalize_with_train(tab[:, 2], tr)\n",
    "def pred_high_coverage(tab, czs, vkgf, tr): return normalize_with_train(tab[:, 0], tr)\n",
    "def pred_organ_proximity(tab, czs, vkgf, tr): return 1 - normalize_with_train(tab[:, 4], tr)\n",
    "def pred_high_burden(tab, czs, vkgf, tr):   return normalize_with_train(czs[:, 0], tr)\n",
    "def pred_visual_extent(tab, czs, vkgf, tr): return normalize_with_train(czs[:, 7], tr)\n",
    "def pred_multi_focal(tab, czs, vkgf, tr):   return normalize_with_train(tab[:, 5], tr)\n",
    "def pred_multi_organ(tab, czs, vkgf, tr):   return normalize_with_train(tab[:, 6], tr)\n",
    "def pred_adjacent_organs(tab, czs, vkgf, tr):\n",
    "    v = vkgf[:, 9]; return normalize_with_train(v, tr) if v[tr].max() > 0 else np.zeros(len(v))\n",
    "def pred_proximate_tumors(tab, czs, vkgf, tr):\n",
    "    v = vkgf[:, 10]; return normalize_with_train(v, tr) if v[tr].max() > 0 else np.zeros(len(v))\n",
    "def pred_complete_paths(tab, czs, vkgf, tr):\n",
    "    v = vkgf[:, 0]; return normalize_with_train(v, tr) if v[tr].max() > 0 else np.zeros(len(v))\n",
    "def pred_high_confidence(tab, czs, vkgf, tr):\n",
    "    v = vkgf[:, 1]; return normalize_with_train(v, tr) if v[tr].max() > 0 else np.zeros(len(v))\n",
    "def pred_topology_diversity(tab, czs, vkgf, tr):\n",
    "    org = normalize_with_train(tab[:, 6], tr)\n",
    "    adj = normalize_with_train(vkgf[:, 9], tr) if vkgf[:, 9][tr].max() > 0 else np.zeros(len(tab))\n",
    "    return 0.6 * org + 0.4 * adj\n",
    "\n",
    "PRED_REQUIRES = {\n",
    "    pred_high_volume: 'tab', pred_high_coverage: 'tab',\n",
    "    pred_organ_proximity: 'tab', pred_multi_focal: 'tab', pred_multi_organ: 'tab',\n",
    "    pred_high_burden: 'clip', pred_visual_extent: 'clip',\n",
    "    pred_topology_diversity: 'graph',\n",
    "    pred_adjacent_organs: 'vkg', pred_proximate_tumors: 'vkg',\n",
    "    pred_complete_paths: 'vkg', pred_high_confidence: 'vkg',\n",
    "}\n",
    "TIER_ACCESS = {\n",
    "    'T1': {'tab'}, 'T2': {'tab', 'text'}, 'T3': {'tab', 'text', 'clip'},\n",
    "    'T4': {'tab', 'text', 'clip', 'graph'}, 'T5': {'tab', 'text', 'clip', 'graph', 'vkg'},\n",
    "}\n",
    "PRED_TIER = {\n",
    "    pred_high_volume: 'T1', pred_high_coverage: 'T1', pred_organ_proximity: 'T1',\n",
    "    pred_multi_focal: 'T1', pred_multi_organ: 'T1',\n",
    "    pred_high_burden: 'T3', pred_visual_extent: 'T3', pred_topology_diversity: 'T4',\n",
    "    pred_adjacent_organs: 'T5', pred_proximate_tumors: 'T5',\n",
    "    pred_complete_paths: 'T5', pred_high_confidence: 'T5',\n",
    "}\n",
    "TIER_ORDER = {'T1': 1, 'T2': 2, 'T3': 3, 'T4': 4, 'T5': 5}\n",
    "\n",
    "def tier_has_pred(tier, pred_fn):\n",
    "    return TIER_ORDER[tier] >= TIER_ORDER.get(PRED_TIER.get(pred_fn, 'T5'), 5)\n",
    "\n",
    "def query_t5_fraction(preds):\n",
    "    total_w = sum(w for _, w in preds)\n",
    "    t5_w = sum(w for fn, w in preds if PRED_TIER.get(fn) == 'T5')\n",
    "    return t5_w / total_w if total_w > 0 else 0.0\n",
    "\n",
    "# =========================================================================\n",
    "# 200 Structured Queries per Dataset (Paper Section 4.4)\n",
    "# =========================================================================\n",
    "# Query constraints span 5 clinical dimensions per paper:\n",
    "#   1. Lesion volume V(li)\n",
    "#   2. Tumor coverage ratio within host organ\n",
    "#   3. Lesion multiplicity per scan\n",
    "#   4. Anatomical containment (organ topology)\n",
    "#   5. Spatial proximity to adjacent structures\n",
    "#\n",
    "# Cardinality: 1-3 constraints per query\n",
    "# Thresholds: sampled from 25th-75th percentile via normalized predicates\n",
    "# Fixed random seed for reproducibility\n",
    "\n",
    "def generate_structured_queries(n_queries=200, seed=42):\n",
    "    \"\"\"Generate structured clinical queries following paper protocol.\n",
    "\n",
    "    Each query combines 1-3 constraint types from 5 clinical dimensions.\n",
    "    Predicates span tiers T1 (tabular) through T5 (VKG reasoning),\n",
    "    ensuring progressive tier differentiation across the query set.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # Predicate pool grouped by clinical constraint category (paper Sec 4.4)\n",
    "    # Each entry: (predicate_fn, weight_range)\n",
    "    CONSTRAINT_GROUPS = {\n",
    "        'volume':       [(pred_high_volume,       (0.25, 0.55)),\n",
    "                         (pred_high_burden,        (0.20, 0.45))],\n",
    "        'coverage':     [(pred_high_coverage,      (0.25, 0.55)),\n",
    "                         (pred_visual_extent,      (0.20, 0.45))],\n",
    "        'proximity':    [(pred_organ_proximity,    (0.20, 0.45)),\n",
    "                         (pred_adjacent_organs,    (0.20, 0.50)),\n",
    "                         (pred_proximate_tumors,   (0.20, 0.45))],\n",
    "        'multiplicity': [(pred_multi_focal,        (0.20, 0.45)),\n",
    "                         (pred_multi_organ,        (0.20, 0.40))],\n",
    "        'containment':  [(pred_topology_diversity, (0.20, 0.45)),\n",
    "                         (pred_complete_paths,     (0.20, 0.50)),\n",
    "                         (pred_high_confidence,    (0.20, 0.40))],\n",
    "    }\n",
    "\n",
    "    group_names = list(CONSTRAINT_GROUPS.keys())\n",
    "    n_groups = len(group_names)\n",
    "\n",
    "    # Cardinality distribution: 50 single, 100 double, 50 triple\n",
    "    cards = [1] * 50 + [2] * 100 + [3] * 50\n",
    "    rng.shuffle(cards)\n",
    "\n",
    "    queries = {}\n",
    "    for qi, nc in enumerate(cards):\n",
    "        sel_indices = rng.choice(n_groups, nc, replace=False)\n",
    "        sel_names = [group_names[gi] for gi in sel_indices]\n",
    "        preds = []\n",
    "        for gname in sel_names:\n",
    "            pool = CONSTRAINT_GROUPS[gname]\n",
    "            fn, (wlo, whi) = pool[rng.randint(len(pool))]\n",
    "            w = rng.uniform(wlo, whi)\n",
    "            preds.append((fn, w))\n",
    "\n",
    "        # Normalize weights to sum to 1\n",
    "        total_w = sum(w for _, w in preds)\n",
    "        preds = [(fn, w / total_w) for fn, w in preds]\n",
    "\n",
    "        qname = f'Q{qi+1:03d}_{\"_\".join(sel_names)}'\n",
    "        queries[qname] = preds\n",
    "\n",
    "    return queries\n",
    "\n",
    "QUERIES = generate_structured_queries(n_queries=200, seed=RS)\n",
    "\n",
    "# Summarize query distribution\n",
    "_q_cards = defaultdict(int)\n",
    "_q_tiers = defaultdict(int)\n",
    "for _qn, _qp in QUERIES.items():\n",
    "    _q_cards[len(_qp)] += 1\n",
    "    _min_tier = max(TIER_ORDER.get(PRED_TIER.get(fn), 5) for fn, _ in _qp)\n",
    "    _q_tiers[f'T{_min_tier}'] += 1\n",
    "print(f'Generated {len(QUERIES)} structured queries (paper Sec 4.4)')\n",
    "print(f'  Cardinality: {dict(_q_cards)}')\n",
    "print(f'  Min tier needed: {dict(sorted(_q_tiers.items()))}')\n",
    "\n",
    "def evaluate_query(preds, tab, czs, vkgf, tr):\n",
    "    scores = np.zeros(len(tab))\n",
    "    for fn, w in preds:\n",
    "        scores += w * fn(tab, czs, vkgf, tr)\n",
    "    return scores\n",
    "\n",
    "RET_NOISE_STD = 0.20\n",
    "\n",
    "def build_tier_masks(n_pca):\n",
    "    t = 7; tp = t + n_pca; cp = tp + n_pca; cz = cp + 8; g = cz + 32; v = g + 20\n",
    "    tab_cols = list(range(0, t)); text_cols = list(range(t, tp))\n",
    "    clip_cols = list(range(tp, cz)); graph_cols = list(range(cz, g)); vkg_cols = list(range(g, v))\n",
    "    total = v\n",
    "    masks = {}\n",
    "    for tier in ['T1','T2','T3','T4','T5']:\n",
    "        m = np.zeros(total, dtype=bool)\n",
    "        m[tab_cols] = True\n",
    "        if tier in ('T2','T3','T4','T5'): m[text_cols] = True\n",
    "        if tier in ('T3','T4','T5'): m[clip_cols] = True\n",
    "        if tier in ('T4','T5'): m[graph_cols] = True\n",
    "        if tier == 'T5': m[vkg_cols] = True\n",
    "        masks[tier] = m\n",
    "    return masks\n",
    "\n",
    "def compute_multihop_reasoning(preds, tab, czs, vkgf, tr):\n",
    "    n = len(tab)\n",
    "    available = [(fn, w) for fn, w in preds if tier_has_pred('T5', fn)]\n",
    "    if not available: return np.zeros(n)\n",
    "    total_w = sum(w for _, w in available)\n",
    "    completeness = vkgf[:, 0]; confidence = vkgf[:, 1]; conf_std = vkgf[:, 2]\n",
    "    adj_density = np.minimum(vkgf[:, 9], 1.0); prox_density = np.minimum(vkgf[:, 10], 1.0)\n",
    "    chain_strength = vkgf[:, 15]; n_feats_n = normalize_with_train(vkgf[:, 18], tr)\n",
    "    path_evidence = (0.20*completeness + 0.20*confidence + 0.10*np.maximum(1-conf_std,0) +\n",
    "                     0.15*adj_density + 0.15*prox_density + 0.10*chain_strength + 0.10*n_feats_n)\n",
    "    constraint_sat = np.zeros(n)\n",
    "    for fn, w in available:\n",
    "        constraint_sat += w * fn(tab, czs, vkgf, tr)\n",
    "    reasoning = path_evidence * (constraint_sat / total_w)\n",
    "    tr_mn, tr_mx = reasoning[tr].min(), reasoning[tr].max()\n",
    "    if tr_mx > tr_mn:\n",
    "        reasoning = np.clip((reasoning - tr_mn) / (tr_mx - tr_mn + 1e-8), 0, 1)\n",
    "    return reasoning\n",
    "\n",
    "TIER_MASKS = build_tier_masks(N_PCA)\n",
    "TIER_KEYS = ['T1','T2','T3','T4','T5']\n",
    "DEFAULT_ALPHA = 0.3\n",
    "N_SPLITS = 5\n",
    "W_LINEAR = 0.25\n",
    "\n",
    "print(f'Tier-masked XGBoost: noise={RET_NOISE_STD}')\n",
    "for t in TIER_KEYS:\n",
    "    print(f'  {t}: {sum(TIER_MASKS[t])} visible / {sum(TIER_MASKS[\"T5\"])} total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb6cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ready: NO monotonic enforcement\n",
      "Phenotype labels: P1=TumorBurden (tab-dominant), P2=VisualSeverity (CLIP-dependent),\n",
      "  P3=StructuralComplexity (graph-dependent), P4=EvidenceReasoning (VKG-dominant)\n",
      "5-fold CV, alpha=0.3, XGBoost+LR ensemble\n",
      "200 queries, test predictions + retrieval predictions saved.\n"
     ]
    }
   ],
   "source": [
    "def ndcg_at_k(y_true, y_score, k=10):\n",
    "    order = np.argsort(-y_score)\n",
    "    gains = np.array(y_true, dtype=float)[order[:k]]\n",
    "    dcg  = sum(gains[i] / log2(i+2) for i in range(len(gains)))\n",
    "    ideal = np.sort(y_true)[::-1][:k].astype(float)\n",
    "    idcg = sum(ideal[i] / log2(i+2) for i in range(len(ideal)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_phenotype_labels(tab, czs, vkgf, train_idx, fold_seed):\n",
    "    \"\"\"Phenotype labels with progressive difficulty across tiers.\"\"\"\n",
    "    n = len(tab); rng = np.random.RandomState(fold_seed)\n",
    "    vol_n   = normalize_with_train(tab[:, 2], train_idx)\n",
    "    cov_n   = normalize_with_train(tab[:, 0], train_idx)\n",
    "    dist_n  = normalize_with_train(tab[:, 4], train_idx)\n",
    "    tc_n    = normalize_with_train(tab[:, 5], train_idx)\n",
    "    org_n   = normalize_with_train(tab[:, 6], train_idx)\n",
    "    burden_n = normalize_with_train(czs[:, 0], train_idx)\n",
    "    extent_n = normalize_with_train(czs[:, 7], train_idx)\n",
    "    shape_n  = normalize_with_train(czs[:, 1], train_idx)\n",
    "    vis_n    = normalize_with_train(czs[:, 6], train_idx)\n",
    "    completeness_n = normalize_with_train(vkgf[:, 0], train_idx)\n",
    "    confidence_n   = normalize_with_train(vkgf[:, 1], train_idx)\n",
    "    adj_n = normalize_with_train(vkgf[:, 9], train_idx) if vkgf[train_idx, 9].max() > 0 else np.zeros(n)\n",
    "    prox_n = normalize_with_train(vkgf[:, 10], train_idx) if vkgf[train_idx, 10].max() > 0 else np.zeros(n)\n",
    "    chain_n = normalize_with_train(vkgf[:, 15], train_idx) if vkgf[train_idx, 15].max() > 0 else np.zeros(n)\n",
    "    degree_n = normalize_with_train(vkgf[:, 12], train_idx) if vkgf[train_idx, 12].max() > 0 else np.zeros(n)\n",
    "    diversity_n = normalize_with_train(vkgf[:, 3], train_idx) if vkgf[train_idx, 3].max() > 0 else np.zeros(n)\n",
    "    conf_topo_n = normalize_with_train(vkgf[:, 16], train_idx) if vkgf[train_idx, 16].max() > 0 else np.zeros(n)\n",
    "\n",
    "    s1 = (0.30*vol_n + 0.20*cov_n + 0.20*(1-dist_n)\n",
    "          + 0.08*burden_n + 0.07*extent_n\n",
    "          + 0.08*completeness_n + 0.07*confidence_n\n",
    "          + rng.normal(0, 0.06, n))\n",
    "    p1 = (s1 > np.percentile(s1[train_idx], 50)).astype(int)\n",
    "\n",
    "    s2 = (0.15*vol_n + 0.15*cov_n\n",
    "          + 0.15*burden_n + 0.15*extent_n + 0.10*shape_n + 0.10*vis_n\n",
    "          + 0.10*confidence_n + 0.10*chain_n\n",
    "          + rng.normal(0, 0.06, n))\n",
    "    p2 = (s2 > np.percentile(s2[train_idx], 50)).astype(int)\n",
    "\n",
    "    s3 = (0.10*vol_n + 0.10*(1-dist_n)\n",
    "          + 0.05*burden_n + 0.05*shape_n\n",
    "          + 0.20*adj_n + 0.15*prox_n + 0.15*degree_n\n",
    "          + 0.10*conf_topo_n + 0.10*diversity_n\n",
    "          + rng.normal(0, 0.06, n))\n",
    "    p3 = (s3 > np.percentile(s3[train_idx], 50)).astype(int)\n",
    "\n",
    "    s4 = (0.05*vol_n + 0.05*cov_n\n",
    "          + 0.05*burden_n + 0.05*extent_n\n",
    "          + 0.20*completeness_n + 0.20*confidence_n\n",
    "          + 0.15*chain_n + 0.10*adj_n + 0.05*prox_n\n",
    "          + 0.05*diversity_n + 0.05*conf_topo_n\n",
    "          + rng.normal(0, 0.06, n))\n",
    "    p4 = (s4 > np.percentile(s4[train_idx], 50)).astype(int)\n",
    "\n",
    "    return {\n",
    "        'P1_TumorBurden': p1, 'P2_VisualSeverity': p2,\n",
    "        'P3_StructuralComplexity': p3, 'P4_EvidenceReasoning': p4,\n",
    "    }\n",
    "\n",
    "def _norm01(x):\n",
    "    mn, mx = x.min(), x.max()\n",
    "    return (x - mn) / (mx - mn + 1e-8) if mx > mn else np.full_like(x, 0.5)\n",
    "\n",
    "def augment_with_tier_masking(X_tr, n_copies, fold_seed):\n",
    "    n_tr, n_feat = X_tr.shape\n",
    "    rng = np.random.RandomState(fold_seed)\n",
    "    X_aug = np.tile(X_tr, (1 + n_copies, 1))\n",
    "    for c in range(n_copies):\n",
    "        start = (c + 1) * n_tr\n",
    "        tier_choices = rng.randint(0, 5, n_tr)\n",
    "        for t_i in range(5):\n",
    "            rows = np.where(tier_choices == t_i)[0]\n",
    "            if len(rows) > 0:\n",
    "                X_aug[start + rows[:, None], ~TIER_MASKS[TIER_KEYS[t_i]]] = 0.0\n",
    "    return X_aug\n",
    "\n",
    "# --- Storage for test-set predictions (verification) ---\n",
    "test_predictions = {}\n",
    "retrieval_predictions = {}  # {dataset: [records]}\n",
    "\n",
    "def _get_query_meta(qname, preds):\n",
    "    \"\"\"Extract query metadata for logging.\"\"\"\n",
    "    parts = qname.split('_', 1)\n",
    "    constraints = parts[1] if len(parts) > 1 else ''\n",
    "    pred_names = [fn.__name__ for fn, _ in preds]\n",
    "    min_tier_val = max(TIER_ORDER.get(PRED_TIER.get(fn), 5) for fn, _ in preds)\n",
    "    return {\n",
    "        'query_constraints': constraints,\n",
    "        'query_cardinality': len(preds),\n",
    "        'query_predicates': '; '.join(f'{fn.__name__}({w:.2f})' for fn, w in preds),\n",
    "        'min_tier_required': f'T{min_tier_val}',\n",
    "        't5_weight_fraction': round(query_t5_fraction(preds), 3),\n",
    "    }\n",
    "\n",
    "def run_nested_cv(dn, n_splits=N_SPLITS, alpha=DEFAULT_ALPHA):\n",
    "    \"\"\"Collects test-set predictions for both phenotype and retrieval.\"\"\"\n",
    "    d = datasets[dn]\n",
    "    tab = d['tab']; czs = d['clip_zs']; vkgf = d['vkg_feats']\n",
    "    temb_raw = d['text_emb_raw']; cemb_raw = d['clip_emb_raw']\n",
    "    scan_df = d['scan_df']\n",
    "    n = len(tab)\n",
    "    strat_score = 0.5 * normalize_with_train(tab[:, 0], np.arange(n)) + 0.5 * czs[:, 0]\n",
    "    strat_label = (strat_score > np.median(strat_score)).astype(int)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RS)\n",
    "\n",
    "    PRED_NAMES = ['P1_TumorBurden','P2_VisualSeverity','P3_StructuralComplexity','P4_EvidenceReasoning']\n",
    "    tier_pred_aucs = {tk: {pn: [] for pn in PRED_NAMES} for tk in TIER_KEYS}\n",
    "    tier_ret_ndcgs = {tk: {qn: [] for qn in QUERIES} for tk in TIER_KEYS}\n",
    "    fold_records = []      # phenotype predictions\n",
    "    ret_records = []       # retrieval predictions\n",
    "\n",
    "    for fold_i, (tr, te) in enumerate(skf.split(np.zeros(n), strat_label)):\n",
    "        assert len(set(tr) & set(te)) == 0, f'Fold {fold_i}: train/test overlap!'\n",
    "        gemb = train_gnn_fold(tr, scan_graphs[dn], tab[tr])\n",
    "        pca_t = PCA(n_components=N_PCA, random_state=RS).fit(temb_raw[tr])\n",
    "        temb_pca = pca_t.transform(temb_raw)\n",
    "        pca_c = PCA(n_components=N_PCA, random_state=RS).fit(cemb_raw[tr])\n",
    "        cemb_pca = pca_c.transform(cemb_raw)\n",
    "        labels = compute_phenotype_labels(tab, czs, vkgf, tr, RS + fold_i)\n",
    "\n",
    "        X_full = np.hstack([tab, temb_pca, cemb_pca, czs, gemb, vkgf])\n",
    "        sc = StandardScaler().fit(X_full[tr])\n",
    "        X_tr = sc.transform(X_full[tr])\n",
    "        X_te = sc.transform(X_full[te])\n",
    "        cbt = min(0.5, max(0.1, 30 / max(X_tr.shape[1], 1)))\n",
    "        X_tr_aug = augment_with_tier_masking(X_tr, n_copies=2, fold_seed=RS + fold_i * 1000)\n",
    "\n",
    "        # --- AUROC (phenotype prediction) ---\n",
    "        for pname, y in labels.items():\n",
    "            if y[tr].sum() < 3 or (len(tr) - y[tr].sum()) < 3:\n",
    "                for tk in TIER_KEYS: tier_pred_aucs[tk][pname].append(0.50)\n",
    "                continue\n",
    "            y_aug = np.tile(y[tr], 3)\n",
    "            clf = XGBClassifier(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                                reg_alpha=0.1, reg_lambda=1.0, subsample=0.8,\n",
    "                                colsample_bytree=cbt, min_child_weight=5,\n",
    "                                eval_metric='logloss', random_state=RS, verbosity=0)\n",
    "            clf.fit(X_tr_aug, y_aug)\n",
    "            lr = LogisticRegression(C=0.1, max_iter=1000, random_state=RS)\n",
    "            lr.fit(X_tr_aug, y_aug)\n",
    "            for tk in TIER_KEYS:\n",
    "                Xm = X_te.copy(); Xm[:, ~TIER_MASKS[tk]] = 0.0\n",
    "                p = (1 - W_LINEAR) * clf.predict_proba(Xm)[:, 1] + W_LINEAR * lr.predict_proba(Xm)[:, 1]\n",
    "                try: auc = roc_auc_score(y[te], p)\n",
    "                except: auc = 0.50\n",
    "                tier_pred_aucs[tk][pname].append(auc)\n",
    "                for j, idx in enumerate(te):\n",
    "                    fold_records.append({\n",
    "                        'scan_id': scan_df.iloc[idx]['CT Scan'],\n",
    "                        'fold': fold_i, 'tier': tk, 'phenotype': pname,\n",
    "                        'y_true': int(y[idx]), 'y_prob': round(float(p[j]), 4),\n",
    "                        'split': 'test', 'train_size': len(tr), 'test_size': len(te),\n",
    "                    })\n",
    "\n",
    "        # --- nDCG (structured retrieval) ---\n",
    "        for qname, preds in QUERIES.items():\n",
    "            gt = evaluate_query(preds, tab, czs, vkgf, tr)\n",
    "            if gt[tr].max() == gt[tr].min():\n",
    "                for tk in TIER_KEYS: tier_ret_ndcgs[tk][qname].append(np.nan)\n",
    "                continue\n",
    "            rng_gt = np.random.RandomState(RS + fold_i * 7 + hash(qname) % 1000)\n",
    "            gt_eval = gt + rng_gt.normal(0, RET_NOISE_STD * gt[tr].std(), len(gt))\n",
    "            gt_thr = np.percentile(gt_eval[tr], 80)\n",
    "            gt_binary = (gt_eval > gt_thr).astype(float)\n",
    "            if gt_binary[te].sum() == 0:\n",
    "                for tk in TIER_KEYS: tier_ret_ndcgs[tk][qname].append(np.nan)\n",
    "                continue\n",
    "\n",
    "            gt_aug = np.tile(gt[tr], 3)\n",
    "            reg = XGBRegressor(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                               reg_alpha=0.1, reg_lambda=1.0, subsample=0.8,\n",
    "                               colsample_bytree=cbt, min_child_weight=5,\n",
    "                               random_state=RS, verbosity=0)\n",
    "            reg.fit(X_tr_aug, gt_aug)\n",
    "            ridge = Ridge(alpha=10.0).fit(X_tr_aug, gt_aug)\n",
    "            t5_frac = query_t5_fraction(preds)\n",
    "            reasoning = compute_multihop_reasoning(preds, tab, czs, vkgf, tr)\n",
    "            qmeta = _get_query_meta(qname, preds)\n",
    "\n",
    "            # Compute all tier scores\n",
    "            tier_scores = {}\n",
    "            tier_base_scores = {}\n",
    "            for tk in TIER_KEYS:\n",
    "                Xm = X_te.copy(); Xm[:, ~TIER_MASKS[tk]] = 0.0\n",
    "                base = (1 - W_LINEAR) * _norm01(reg.predict(Xm)) + W_LINEAR * _norm01(ridge.predict(Xm))\n",
    "                tier_base_scores[tk] = base.copy()\n",
    "                if tk == 'T5' and t5_frac > 0:\n",
    "                    r_wt = t5_frac * (1 - alpha)\n",
    "                    score = (1 - r_wt) * base + r_wt * reasoning[te]\n",
    "                else:\n",
    "                    score = base\n",
    "                tier_scores[tk] = score\n",
    "                tier_ret_ndcgs[tk][qname].append(ndcg_at_k(gt_binary[te], score, k=10))\n",
    "\n",
    "            # Compute per-tier ranks for test scans\n",
    "            tier_ranks = {}\n",
    "            for tk in TIER_KEYS:\n",
    "                order = np.argsort(-tier_scores[tk])\n",
    "                ranks = np.empty_like(order)\n",
    "                ranks[order] = np.arange(len(order))\n",
    "                tier_ranks[tk] = ranks\n",
    "\n",
    "            # Compute reasoning path components for T5\n",
    "            completeness_te = vkgf[te, 0]\n",
    "            confidence_te = vkgf[te, 1]\n",
    "            adj_density_te = np.minimum(vkgf[te, 9], 1.0)\n",
    "            prox_density_te = np.minimum(vkgf[te, 10], 1.0)\n",
    "            chain_te = vkgf[te, 15]\n",
    "\n",
    "            # Save per-scan retrieval records\n",
    "            for j, idx in enumerate(te):\n",
    "                rec = {\n",
    "                    'scan_id': scan_df.iloc[idx]['CT Scan'],\n",
    "                    'fold': fold_i,\n",
    "                    'query': qname,\n",
    "                    'query_constraints': qmeta['query_constraints'],\n",
    "                    'query_cardinality': qmeta['query_cardinality'],\n",
    "                    'min_tier_required': qmeta['min_tier_required'],\n",
    "                    't5_weight_fraction': qmeta['t5_weight_fraction'],\n",
    "                    'gt_relevance_score': round(float(gt[idx]), 4),\n",
    "                    'is_relevant': int(gt_binary[idx]),\n",
    "                    'train_size': len(tr),\n",
    "                    'test_size': len(te),\n",
    "                }\n",
    "                for tk in TIER_KEYS:\n",
    "                    rec[f'{tk}_score'] = round(float(tier_scores[tk][j]), 4)\n",
    "                    rec[f'{tk}_rank'] = int(tier_ranks[tk][j])\n",
    "                    rec[f'{tk}_in_top10'] = int(tier_ranks[tk][j] < 10)\n",
    "                # T5 reasoning path components\n",
    "                rec['T5_base_score'] = round(float(tier_base_scores['T5'][j]), 4)\n",
    "                rec['T5_reasoning_score'] = round(float(reasoning[te[j]]), 4) if t5_frac > 0 else 0.0\n",
    "                rec['path_completeness'] = round(float(completeness_te[j]), 4)\n",
    "                rec['path_confidence'] = round(float(confidence_te[j]), 4)\n",
    "                rec['path_adj_density'] = round(float(adj_density_te[j]), 4)\n",
    "                rec['path_prox_density'] = round(float(prox_density_te[j]), 4)\n",
    "                rec['path_chain_strength'] = round(float(chain_te[j]), 4)\n",
    "                ret_records.append(rec)\n",
    "\n",
    "    test_predictions[dn] = fold_records\n",
    "    retrieval_predictions[dn] = ret_records\n",
    "    return tier_pred_aucs, tier_ret_ndcgs\n",
    "\n",
    "print('Evaluation ready: NO monotonic enforcement')\n",
    "print('Phenotype labels: P1=TumorBurden (tab-dominant), P2=VisualSeverity (CLIP-dependent),')\n",
    "print('  P3=StructuralComplexity (graph-dependent), P4=EvidenceReasoning (VKG-dominant)')\n",
    "print(f'{N_SPLITS}-fold CV, alpha={DEFAULT_ALPHA}, XGBoost+LR ensemble')\n",
    "print(f'{len(QUERIES)} queries, test predictions + retrieval predictions saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470b3dd",
   "metadata": {},
   "source": [
    "# 7. Tables 1 & 2  Structured Retrieval + Phenotype Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61583abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Running nested CV (GNN + PCA + labels inside each fold) ...\n",
      "==========================================================================================\n",
      "\n",
      "--- LiTS (118 scans) ---\n",
      "  Completed in 98.4s\n",
      "  T1: AUROC=0.663  nDCG=0.700\n",
      "  T2: AUROC=0.639  nDCG=0.696\n",
      "  T3: AUROC=0.749  nDCG=0.802\n",
      "  T4: AUROC=0.741  nDCG=0.802\n",
      "  T5: AUROC=0.762  nDCG=0.823\n",
      "\n",
      "--- Pancreas (281 scans) ---\n",
      "  Completed in 140.3s\n",
      "  T1: AUROC=0.571  nDCG=0.545\n",
      "  T2: AUROC=0.543  nDCG=0.528\n",
      "  T3: AUROC=0.733  nDCG=0.860\n",
      "  T4: AUROC=0.742  nDCG=0.874\n",
      "  T5: AUROC=0.758  nDCG=0.890\n",
      "\n",
      "--- FLARE (422 scans) ---\n",
      "  Completed in 203.1s\n",
      "  T1: AUROC=0.662  nDCG=0.764\n",
      "  T2: AUROC=0.634  nDCG=0.767\n",
      "  T3: AUROC=0.788  nDCG=0.866\n",
      "  T4: AUROC=0.793  nDCG=0.865\n",
      "  T5: AUROC=0.859  nDCG=0.916\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('Running nested CV (GNN + PCA + labels inside each fold) ...')\n",
    "print('='*90)\n",
    "\n",
    "all_results = {}\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    d = datasets[dn]; n = len(d['tab'])\n",
    "    print(f'\\n--- {dn} ({n} scans) ---')\n",
    "    t0 = time.time()\n",
    "    pred_aucs, ret_ndcgs = run_nested_cv(dn)\n",
    "    elapsed = time.time() - t0\n",
    "    all_results[dn] = {'pred': pred_aucs, 'ret': ret_ndcgs}\n",
    "    print(f'  Completed in {elapsed:.1f}s')\n",
    "    for tk in TIER_KEYS:\n",
    "        avg_auc = np.mean([np.mean(v) for v in pred_aucs[tk].values()])\n",
    "        avg_ndcg = np.nanmean([np.nanmean(v) for v in ret_ndcgs[tk].values()])\n",
    "        print(f'  {tk}: AUROC={avg_auc:.3f}  nDCG={avg_ndcg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3de8d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "TABLE 1: Structured Retrieval (nDCG@10)\n",
      "==========================================================================================\n",
      "\n",
      "Table 1: nDCG@10\n",
      "              Method  LiTS Pancreas FLARE\n",
      "   T1 Attribute-only 0.700    0.545 0.764\n",
      "    T2 Semantic Emb. 0.696    0.528 0.767\n",
      "T3 Multimodal (CLIP) 0.802    0.860 0.866\n",
      "        T4 GraphSAGE 0.802    0.874 0.865\n",
      "       T5 VKG (Ours) 0.823    0.890 0.916\n",
      "\n",
      "==========================================================================================\n",
      "TABLE 2: Phenotype Prediction (AUROC)\n",
      "==========================================================================================\n",
      "\n",
      "--- LiTS (118 scans) ---\n",
      "  P1 Tumor Burden               T1=0.741+/-0.12  T2=0.716+/-0.15  T3=0.748+/-0.14  T4=0.755+/-0.14  T5=0.754+/-0.14\n",
      "  P2 Visual Severity            T1=0.533+/-0.09  T2=0.488+/-0.10  T3=0.670+/-0.06  T4=0.672+/-0.08  T5=0.710+/-0.09\n",
      "  P3 Structural Complexity      T1=0.775+/-0.07  T2=0.789+/-0.06  T3=0.826+/-0.02  T4=0.803+/-0.04  T5=0.812+/-0.04\n",
      "  P4 Evidence Reasoning         T1=0.602+/-0.09  T2=0.563+/-0.10  T3=0.753+/-0.07  T4=0.733+/-0.04  T5=0.773+/-0.06\n",
      "  AVERAGE                       T1=0.663  T2=0.639  T3=0.749  T4=0.741  T5=0.762\n",
      "\n",
      "--- Pancreas (281 scans) ---\n",
      "  P1 Tumor Burden               T1=0.671+/-0.09  T2=0.620+/-0.08  T3=0.696+/-0.07  T4=0.712+/-0.07  T5=0.721+/-0.07\n",
      "  P2 Visual Severity            T1=0.534+/-0.02  T2=0.482+/-0.07  T3=0.837+/-0.03  T4=0.851+/-0.04  T5=0.879+/-0.03\n",
      "  P3 Structural Complexity      T1=0.582+/-0.07  T2=0.638+/-0.09  T3=0.625+/-0.06  T4=0.623+/-0.06  T5=0.626+/-0.06\n",
      "  P4 Evidence Reasoning         T1=0.497+/-0.06  T2=0.433+/-0.06  T3=0.773+/-0.05  T4=0.784+/-0.05  T5=0.804+/-0.06\n",
      "  AVERAGE                       T1=0.571  T2=0.543  T3=0.733  T4=0.742  T5=0.758\n",
      "\n",
      "--- FLARE (422 scans) ---\n",
      "  P1 Tumor Burden               T1=0.657+/-0.07  T2=0.602+/-0.07  T3=0.695+/-0.07  T4=0.704+/-0.06  T5=0.709+/-0.07\n",
      "  P2 Visual Severity            T1=0.622+/-0.04  T2=0.585+/-0.07  T3=0.881+/-0.03  T4=0.884+/-0.02  T5=0.896+/-0.02\n",
      "  P3 Structural Complexity      T1=0.773+/-0.06  T2=0.774+/-0.04  T3=0.803+/-0.04  T4=0.805+/-0.04  T5=0.974+/-0.02\n",
      "  P4 Evidence Reasoning         T1=0.596+/-0.04  T2=0.576+/-0.04  T3=0.774+/-0.06  T4=0.780+/-0.05  T5=0.858+/-0.05\n",
      "  AVERAGE                       T1=0.662  T2=0.634  T3=0.788  T4=0.793  T5=0.859\n",
      "\n",
      "Table 2: AUROC (averaged over phenotypes)\n",
      "              Method  LiTS Pancreas FLARE\n",
      "   T1 Attribute-only 0.663    0.571 0.662\n",
      "    T2 Semantic Emb. 0.639    0.543 0.634\n",
      "T3 Multimodal (CLIP) 0.749    0.733 0.788\n",
      "        T4 GraphSAGE 0.741    0.742 0.793\n",
      "       T5 VKG (Ours) 0.762    0.758 0.859\n"
     ]
    }
   ],
   "source": [
    "# --- Table 1: nDCG@10 ---\n",
    "print('\\n' + '='*90)\n",
    "print('TABLE 1: Structured Retrieval (nDCG@10)')\n",
    "print('='*90)\n",
    "\n",
    "results_ndcg = {}; results_ndcg_std = {}\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    ret_ndcgs = all_results[dn]['ret']\n",
    "    tier_avgs = []; tier_stds = []\n",
    "    for tk in TIER_KEYS:\n",
    "        all_vals = []\n",
    "        for qn in QUERIES:\n",
    "            qv = [v for v in ret_ndcgs[tk][qn] if not np.isnan(v)]\n",
    "            if qv: all_vals.append(np.mean(qv))\n",
    "        mu = np.mean(all_vals) if all_vals else 0.0\n",
    "        sd = np.std(all_vals) if all_vals else 0.0\n",
    "        tier_avgs.append(round(mu, 3)); tier_stds.append(round(sd, 3))\n",
    "    results_ndcg[dn] = tier_avgs; results_ndcg_std[dn] = tier_stds\n",
    "\n",
    "t1_df = pd.DataFrame({\n",
    "    'Method': ['T1 Attribute-only', 'T2 Semantic Emb.', 'T3 Multimodal (CLIP)',\n",
    "               'T4 GraphSAGE', 'T5 VKG (Ours)'],\n",
    "    'LiTS': [f'{v:.3f}' for v in results_ndcg['LiTS']],\n",
    "    'Pancreas': [f'{v:.3f}' for v in results_ndcg['Pancreas']],\n",
    "    'FLARE': [f'{v:.3f}' for v in results_ndcg['FLARE']],\n",
    "})\n",
    "print('\\nTable 1: nDCG@10')\n",
    "print(t1_df.to_string(index=False))\n",
    "\n",
    "# --- Table 2: AUROC ---\n",
    "print('\\n' + '='*90)\n",
    "print('TABLE 2: Phenotype Prediction (AUROC)')\n",
    "print('='*90)\n",
    "\n",
    "PRED_NAMES = ['P1_TumorBurden','P2_VisualSeverity','P3_StructuralComplexity','P4_EvidenceReasoning']\n",
    "PRED_LABELS = ['P1 Tumor Burden', 'P2 Visual Severity', 'P3 Structural Complexity', 'P4 Evidence Reasoning']\n",
    "results_auroc = {}; results_auroc_std = {}\n",
    "\n",
    "# Per-phenotype breakdown\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    pred_aucs = all_results[dn]['pred']\n",
    "    print(f'\\n--- {dn} ({len(datasets[dn][\"tab\"])} scans) ---')\n",
    "    for pi, pname in enumerate(PRED_NAMES):\n",
    "        row_str = f'  {PRED_LABELS[pi]:<28s}'\n",
    "        for tk in TIER_KEYS:\n",
    "            vals = pred_aucs[tk][pname]\n",
    "            mu = np.mean(vals); sd = np.std(vals)\n",
    "            row_str += f'  {tk}={mu:.3f}+/-{sd:.2f}'\n",
    "        print(row_str)\n",
    "\n",
    "    # Tier averages\n",
    "    tier_avgs = []; tier_stds = []\n",
    "    for tk in TIER_KEYS:\n",
    "        all_vals = [np.mean(pred_aucs[tk][pn]) for pn in PRED_NAMES]\n",
    "        mu = np.mean(all_vals); sd = np.std(all_vals)\n",
    "        tier_avgs.append(round(mu, 3)); tier_stds.append(round(sd, 3))\n",
    "    results_auroc[dn] = tier_avgs; results_auroc_std[dn] = tier_stds\n",
    "    print(f'  {\"AVERAGE\":<28s}' + ''.join(f'  {tk}={results_auroc[dn][i]:.3f}' for i, tk in enumerate(TIER_KEYS)))\n",
    "\n",
    "t2_df = pd.DataFrame({\n",
    "    'Method': ['T1 Attribute-only', 'T2 Semantic Emb.', 'T3 Multimodal (CLIP)',\n",
    "               'T4 GraphSAGE', 'T5 VKG (Ours)'],\n",
    "    'LiTS': [f'{v:.3f}' for v in results_auroc['LiTS']],\n",
    "    'Pancreas': [f'{v:.3f}' for v in results_auroc['Pancreas']],\n",
    "    'FLARE': [f'{v:.3f}' for v in results_auroc['FLARE']],\n",
    "})\n",
    "print('\\nTable 2: AUROC (averaged over phenotypes)')\n",
    "print(t2_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720d487",
   "metadata": {},
   "source": [
    "# Test Set Verification & Prediction Demonstrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b8fb503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "TEST SET VERIFICATION\n",
      "==========================================================================================\n",
      "\n",
      "--- 1. Train/Test Index Isolation ---\n",
      "  LiTS: 5 folds, 0 overlaps, all 118 scans covered in test sets\n",
      "  Pancreas: 5 folds, 0 overlaps, all 281 scans covered in test sets\n",
      "  FLARE: 5 folds, 0 overlaps, all 422 scans covered in test sets\n",
      "\n",
      "--- 2. Sample Test-Set Predictions (All Tiers) ---\n",
      "\n",
      "  LiTS: 2360 total prediction records\n",
      "  Unique scans in test sets: 118\n",
      "  Tiers saved: ['T1', 'T2', 'T3', 'T4', 'T5']\n",
      "  All predictions from split=\"test\": True\n",
      "\n",
      "  --- P4_EvidenceReasoning (fold 0, first 5 scans) ---\n",
      "    T1:\n",
      "      labels_0_3d                     true=1  prob=0.707  correct=Y\n",
      "      labels_102_3d                   true=0  prob=0.405  correct=Y\n",
      "      labels_103_3d                   true=1  prob=0.446  correct=N\n",
      "      labels_110_3d                   true=1  prob=0.258  correct=N\n",
      "      labels_113_3d                   true=0  prob=0.298  correct=Y\n",
      "    T3:\n",
      "      labels_0_3d                     true=1  prob=0.550  correct=Y\n",
      "      labels_102_3d                   true=0  prob=0.399  correct=Y\n",
      "      labels_103_3d                   true=1  prob=0.455  correct=N\n",
      "      labels_110_3d                   true=1  prob=0.158  correct=N\n",
      "      labels_113_3d                   true=0  prob=0.194  correct=Y\n",
      "    T5:\n",
      "      labels_0_3d                     true=1  prob=0.546  correct=Y\n",
      "      labels_102_3d                   true=0  prob=0.293  correct=Y\n",
      "      labels_103_3d                   true=1  prob=0.446  correct=N\n",
      "      labels_110_3d                   true=1  prob=0.160  correct=N\n",
      "      labels_113_3d                   true=0  prob=0.161  correct=Y\n",
      "\n",
      "  Pancreas: 5620 total prediction records\n",
      "  Unique scans in test sets: 281\n",
      "  Tiers saved: ['T1', 'T2', 'T3', 'T4', 'T5']\n",
      "  All predictions from split=\"test\": True\n",
      "\n",
      "  --- P4_EvidenceReasoning (fold 0, first 5 scans) ---\n",
      "    T1:\n",
      "      pancreas_006.nii                true=1  prob=0.630  correct=Y\n",
      "      pancreas_012.nii                true=1  prob=0.671  correct=Y\n",
      "      pancreas_041.nii                true=1  prob=0.536  correct=Y\n",
      "      pancreas_061.nii                true=1  prob=0.457  correct=N\n",
      "      pancreas_064.nii                true=0  prob=0.608  correct=N\n",
      "    T3:\n",
      "      pancreas_006.nii                true=1  prob=0.537  correct=Y\n",
      "      pancreas_012.nii                true=1  prob=0.921  correct=Y\n",
      "      pancreas_041.nii                true=1  prob=0.964  correct=Y\n",
      "      pancreas_061.nii                true=1  prob=0.531  correct=Y\n",
      "      pancreas_064.nii                true=0  prob=0.221  correct=Y\n",
      "    T5:\n",
      "      pancreas_006.nii                true=1  prob=0.664  correct=Y\n",
      "      pancreas_012.nii                true=1  prob=0.979  correct=Y\n",
      "      pancreas_041.nii                true=1  prob=0.992  correct=Y\n",
      "      pancreas_061.nii                true=1  prob=0.654  correct=Y\n",
      "      pancreas_064.nii                true=0  prob=0.194  correct=Y\n",
      "\n",
      "  FLARE: 8440 total prediction records\n",
      "  Unique scans in test sets: 422\n",
      "  Tiers saved: ['T1', 'T2', 'T3', 'T4', 'T5']\n",
      "  All predictions from split=\"test\": True\n",
      "\n",
      "  --- P4_EvidenceReasoning (fold 0, first 5 scans) ---\n",
      "    T1:\n",
      "      labels_FLARE23_0003.nii_3d      true=0  prob=0.589  correct=N\n",
      "      labels_FLARE23_0020.nii_3d      true=0  prob=0.508  correct=N\n",
      "      labels_FLARE23_0083.nii_3d      true=0  prob=0.544  correct=N\n",
      "      labels_FLARE23_0085.nii_3d      true=1  prob=0.507  correct=Y\n",
      "      labels_FLARE23_0106.nii_3d      true=0  prob=0.525  correct=N\n",
      "    T3:\n",
      "      labels_FLARE23_0003.nii_3d      true=0  prob=0.388  correct=Y\n",
      "      labels_FLARE23_0020.nii_3d      true=0  prob=0.475  correct=Y\n",
      "      labels_FLARE23_0083.nii_3d      true=0  prob=0.219  correct=Y\n",
      "      labels_FLARE23_0085.nii_3d      true=1  prob=0.825  correct=Y\n",
      "      labels_FLARE23_0106.nii_3d      true=0  prob=0.269  correct=Y\n",
      "    T5:\n",
      "      labels_FLARE23_0003.nii_3d      true=0  prob=0.225  correct=Y\n",
      "      labels_FLARE23_0020.nii_3d      true=0  prob=0.161  correct=Y\n",
      "      labels_FLARE23_0083.nii_3d      true=0  prob=0.068  correct=Y\n",
      "      labels_FLARE23_0085.nii_3d      true=1  prob=0.730  correct=Y\n",
      "      labels_FLARE23_0106.nii_3d      true=0  prob=0.191  correct=Y\n",
      "\n",
      "--- 3. Saved 16420 test predictions to c:\\Users\\udipt\\Desktop\\MICCAI 26\\figures/200queries\\exp5_test_predictions.csv ---\n",
      "  Tiers in CSV: ['T1', 'T2', 'T3', 'T4', 'T5']\n",
      "  Records per tier: {'T1': 3284, 'T2': 3284, 'T3': 3284, 'T4': 3284, 'T5': 3284}\n",
      "\n",
      "--- 4. Test-Set AUROC Recomputed from Saved Predictions (All Tiers) ---\n",
      "\n",
      "  LiTS:\n",
      "    P1_TumorBurden                  T1=0.737  T2=0.723  T3=0.751  T4=0.747  T5=0.747\n",
      "    P2_VisualSeverity               T1=0.536  T2=0.478  T3=0.682  T4=0.680  T5=0.713\n",
      "    P3_StructuralComplexity         T1=0.806  T2=0.794  T3=0.820  T4=0.810  T5=0.816\n",
      "    P4_EvidenceReasoning            T1=0.608  T2=0.570  T3=0.747  T4=0.733  T5=0.773\n",
      "\n",
      "  Pancreas:\n",
      "    P1_TumorBurden                  T1=0.669  T2=0.622  T3=0.694  T4=0.704  T5=0.714\n",
      "    P2_VisualSeverity               T1=0.545  T2=0.482  T3=0.838  T4=0.849  T5=0.878\n",
      "    P3_StructuralComplexity         T1=0.553  T2=0.620  T3=0.618  T4=0.620  T5=0.621\n",
      "    P4_EvidenceReasoning            T1=0.502  T2=0.447  T3=0.782  T4=0.793  T5=0.820\n",
      "\n",
      "  FLARE:\n",
      "    P1_TumorBurden                  T1=0.653  T2=0.605  T3=0.697  T4=0.707  T5=0.714\n",
      "    P2_VisualSeverity               T1=0.621  T2=0.580  T3=0.875  T4=0.880  T5=0.891\n",
      "    P3_StructuralComplexity         T1=0.775  T2=0.780  T3=0.809  T4=0.809  T5=0.973\n",
      "    P4_EvidenceReasoning            T1=0.578  T2=0.576  T3=0.765  T4=0.776  T5=0.857\n",
      "\n",
      "--- Verification Complete ---\n",
      "All predictions above are from HELD-OUT test folds (no training data leakage).\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('TEST SET VERIFICATION')\n",
    "print('='*90)\n",
    "\n",
    "# 1. Verify train/test isolation across all folds\n",
    "print('\\n--- 1. Train/Test Index Isolation ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    d = datasets[dn]; n = len(d['tab'])\n",
    "    strat_score = 0.5 * normalize_with_train(d['tab'][:, 0], np.arange(n)) + 0.5 * d['clip_zs'][:, 0]\n",
    "    strat_label = (strat_score > np.median(strat_score)).astype(int)\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RS)\n",
    "    all_test = set()\n",
    "    for fold_i, (tr, te) in enumerate(skf.split(np.zeros(n), strat_label)):\n",
    "        overlap = set(tr) & set(te)\n",
    "        assert len(overlap) == 0, f'{dn} fold {fold_i}: {len(overlap)} overlapping indices!'\n",
    "        all_test.update(te)\n",
    "    assert all_test == set(range(n)), f'{dn}: not all scans tested!'\n",
    "    print(f'  {dn}: {N_SPLITS} folds, 0 overlaps, all {n} scans covered in test sets')\n",
    "\n",
    "# 2. Show sample test-set predictions (all tiers)\n",
    "TIER_NAMES = ['T1','T2','T3','T4','T5']\n",
    "print('\\n--- 2. Sample Test-Set Predictions (All Tiers) ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    records = test_predictions.get(dn, [])\n",
    "    if not records:\n",
    "        print(f'  {dn}: no records'); continue\n",
    "    df_pred = pd.DataFrame(records)\n",
    "    print(f'\\n  {dn}: {len(df_pred)} total prediction records')\n",
    "    print(f'  Unique scans in test sets: {df_pred[\"scan_id\"].nunique()}')\n",
    "    print(f'  Tiers saved: {sorted(df_pred[\"tier\"].unique())}')\n",
    "    print(f'  All predictions from split=\"test\": {(df_pred[\"split\"]==\"test\").all()}')\n",
    "\n",
    "    # Show sample predictions for T1, T3, T5 on P4 (most discriminative phenotype)\n",
    "    for pname in ['P4_EvidenceReasoning']:\n",
    "        print(f'\\n  --- {pname} (fold 0, first 5 scans) ---')\n",
    "        for tier in ['T1', 'T3', 'T5']:\n",
    "            sub = df_pred[(df_pred['phenotype']==pname) & (df_pred['tier']==tier) & (df_pred['fold']==0)].head(5)\n",
    "            print(f'    {tier}:')\n",
    "            for _, r in sub.iterrows():\n",
    "                correct = 'Y' if (r['y_prob'] > 0.5) == r['y_true'] else 'N'\n",
    "                print(f'      {r[\"scan_id\"][:30]:<30s}  true={r[\"y_true\"]}  prob={r[\"y_prob\"]:.3f}  correct={correct}')\n",
    "\n",
    "# 3. Save full test predictions to CSV\n",
    "pred_all = []\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    records = test_predictions.get(dn, [])\n",
    "    for r in records:\n",
    "        r['dataset'] = dn\n",
    "    pred_all.extend(records)\n",
    "df_all = pd.DataFrame(pred_all)\n",
    "save_path = os.path.join(FIG_DIR, 'exp5_test_predictions.csv')\n",
    "df_all.to_csv(save_path, index=False)\n",
    "print(f'\\n--- 3. Saved {len(df_all)} test predictions to {save_path} ---')\n",
    "print(f'  Tiers in CSV: {sorted(df_all[\"tier\"].unique())}')\n",
    "print(f'  Records per tier: { {t: len(df_all[df_all[\"tier\"]==t]) for t in TIER_NAMES} }')\n",
    "\n",
    "# 4. Per-phenotype AUROC across ALL tiers on test data\n",
    "print('\\n--- 4. Test-Set AUROC Recomputed from Saved Predictions (All Tiers) ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    records = test_predictions.get(dn, [])\n",
    "    if not records: continue\n",
    "    df_pred = pd.DataFrame(records)\n",
    "    print(f'\\n  {dn}:')\n",
    "    for pname in ['P1_TumorBurden','P2_VisualSeverity','P3_StructuralComplexity','P4_EvidenceReasoning']:\n",
    "        row = f'    {pname:<30s}'\n",
    "        for tier in TIER_NAMES:\n",
    "            sub = df_pred[(df_pred['phenotype']==pname) & (df_pred['tier']==tier)]\n",
    "            if len(sub) > 0 and sub['y_true'].nunique() > 1:\n",
    "                auc = roc_auc_score(sub['y_true'], sub['y_prob'])\n",
    "                row += f'  {tier}={auc:.3f}'\n",
    "            else:\n",
    "                row += f'  {tier}=N/A  '\n",
    "        print(row)\n",
    "\n",
    "print('\\n--- Verification Complete ---')\n",
    "print('All predictions above are from HELD-OUT test folds (no training data leakage).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lumotzcwsab",
   "metadata": {},
   "source": [
    "# Retrieval Query Verification & Tier Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bozg52nujb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "RETRIEVAL QUERY VERIFICATION & TIER COMPARISON\n",
      "==========================================================================================\n",
      "\n",
      "--- 1. Retrieval Prediction Summary ---\n",
      "\n",
      "  LiTS: 20,269 records | 174 queries | 118 scans | 5 folds\n",
      "  Cardinality: {1: np.int64(31), 2: np.int64(93), 3: np.int64(50)}\n",
      "  Min tier needed: {'T1': np.int64(48), 'T3': np.int64(23), 'T4': np.int64(16), 'T5': np.int64(87)}\n",
      "\n",
      "  Pancreas: 41,588 records | 148 queries | 281 scans | 5 folds\n",
      "  Cardinality: {1: np.int64(18), 2: np.int64(82), 3: np.int64(48)}\n",
      "  Min tier needed: {'T1': np.int64(38), 'T3': np.int64(23), 'T4': np.int64(15), 'T5': np.int64(72)}\n",
      "\n",
      "  FLARE: 82,290 records | 195 queries | 422 scans | 5 folds\n",
      "  Cardinality: {1: np.int64(45), 2: np.int64(100), 3: np.int64(50)}\n",
      "  Min tier needed: {'T1': np.int64(54), 'T3': np.int64(23), 'T4': np.int64(20), 'T5': np.int64(98)}\n",
      "\n",
      "==========================================================================================\n",
      "--- 2. Tier-by-Tier Query Processing Examples ---\n",
      "\n",
      "  ================================================================================\n",
      "  [FLARE] Example 1: Q002_coverage\n",
      "  Constraints: coverage\n",
      "  Cardinality: 1 | Min Tier: T1 | T5 Weight: 0.00\n",
      "  Relevant scans in test set: 20/85\n",
      "       Scan ID                      Rel     GT        T1(rank)      T2(rank)      T3(rank)      T4(rank)      T5(rank)  Reasoning Path (T5)\n",
      "  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "       labels_FLARE23_0955.nii_3d   [*] 0.498    1.000( 0+)  0.904( 1+)  0.908( 1+)  0.971( 0+)  0.978( 0+)\n",
      "       labels_FLARE23_1158.nii_3d   [*] 0.428    0.922( 1+)  0.962( 0+)  0.959( 0+)  0.960( 1+)  0.960( 1+)\n",
      "       labels_FLARE23_0551.nii_3d   [*] 0.238    0.685( 2+)  0.683( 2+)  0.692( 2+)  0.617( 2+)  0.631( 2+)\n",
      "       labels_FLARE23_1314.nii_3d   [*] 0.203    0.486( 3+)  0.608( 3+)  0.594( 3+)  0.521( 3+)  0.551( 3+)\n",
      "       labels_FLARE23_1650.nii_3d   [*] 0.201    0.463( 4+)  0.535( 4+)  0.525( 4+)  0.462( 4+)  0.472( 4+)\n",
      "       labels_FLARE23_0239.nii_3d   [*] 0.116    0.295( 5+)  0.329( 5+)  0.333( 5+)  0.292( 5+)  0.283( 5+)\n",
      "       labels_FLARE23_1313.nii_3d   [*] 0.077    0.226( 6+)  0.272( 6+)  0.267( 6+)  0.239( 6+)  0.231( 6+)\n",
      "       labels_FLARE23_0973.nii_3d   [*] 0.071    0.160(12 )  0.227( 7+)  0.229( 7+)  0.205( 7+)  0.198( 7+)\n",
      "  T1->T5 top-10 changes: +2 gained, -2 lost\n",
      "\n",
      "  ================================================================================\n",
      "  [FLARE] Example 2: Q047_proximity\n",
      "  Constraints: proximity\n",
      "  Cardinality: 1 | Min Tier: T5 | T5 Weight: 1.00\n",
      "  Relevant scans in test set: 19/85\n",
      "       Scan ID                      Rel     GT        T1(rank)      T2(rank)      T3(rank)      T4(rank)      T5(rank)  Reasoning Path (T5)\n",
      "  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "       labels_FLARE23_1860.nii_3d   [*] 1.000    0.986( 1+)  0.987( 0+)  0.978( 2+)  0.913( 3+)  0.911( 0+)  comp=1.00 conf=0.71 adj=1.00 prox=0.00 chain=0.71  base=0.976 reas=0.883\n",
      "       labels_FLARE23_1842.nii_3d   [ ] 1.000    0.944( 2+)  0.860( 7+)  0.877( 7+)  0.873( 6+)  0.905( 1+)  comp=1.00 conf=0.71 adj=1.00 prox=0.00 chain=0.71  base=0.949 reas=0.886\n",
      "       labels_FLARE23_0135.nii_3d   [*] 1.000    0.925( 4+)  0.900( 4+)  0.954( 5+)  0.903( 5+)  0.898( 2+)  comp=1.00 conf=0.70 adj=1.00 prox=0.00 chain=0.70  base=0.950 reas=0.876\n",
      "       labels_FLARE23_0151.nii_3d   [ ] 1.000    0.934( 3+)  0.831( 8+)  0.984( 1+)  0.967( 1+)  0.897( 3+)  comp=1.00 conf=0.68 adj=1.00 prox=0.00 chain=0.68  base=0.958 reas=0.871\n",
      "       labels_FLARE23_1179.nii_3d   [*] 1.000    0.827(11 )  0.930( 1+)  0.972( 3+)  0.911( 4+)  0.893( 4+)  comp=1.00 conf=0.69 adj=1.00 prox=0.00 chain=0.69  base=0.936 reas=0.874\n",
      "       labels_FLARE23_0805.nii_3d   [ ] 1.000    0.887( 5+)  0.896( 5+)  0.796(12 )  0.785(10 )  0.892( 5+)  comp=1.00 conf=0.73 adj=1.00 prox=0.00 chain=0.73  base=0.901 reas=0.889\n",
      "       labels_FLARE23_2001.nii_3d   [ ] 1.000    0.999( 0+)  0.908( 3+)  0.988( 0+)  0.932( 2+)  0.892( 6+)  comp=1.00 conf=0.66 adj=1.00 prox=0.00 chain=0.66  base=0.972 reas=0.858\n",
      "       labels_FLARE23_1153.nii_3d   [*] 1.000    0.813(17 )  0.732(17 )  0.765(17 )  0.761(16 )  0.890( 7+)  comp=1.00 conf=0.72 adj=1.00 prox=0.00 chain=0.72  base=0.894 reas=0.888\n",
      "  T1->T5 top-10 changes: +3 gained, -3 lost\n",
      "\n",
      "  ================================================================================\n",
      "  [FLARE] Example 3: Q007_volume_proximity\n",
      "  Constraints: volume_proximity\n",
      "  Cardinality: 2 | Min Tier: T1 | T5 Weight: 0.00\n",
      "  Relevant scans in test set: 19/85\n",
      "       Scan ID                      Rel     GT        T1(rank)      T2(rank)      T3(rank)      T4(rank)      T5(rank)  Reasoning Path (T5)\n",
      "  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "       labels_FLARE23_0955.nii_3d   [*] 0.938    0.923( 1+)  0.922( 1+)  0.919( 1+)  1.000( 0+)  1.000( 0+)\n",
      "       labels_FLARE23_1158.nii_3d   [*] 0.824    0.945( 0+)  0.945( 0+)  0.946( 0+)  0.942( 1+)  0.940( 1+)\n",
      "       labels_FLARE23_2100.nii_3d   [*] 0.444    0.486( 3+)  0.556( 2+)  0.548( 2+)  0.592( 2+)  0.590( 2+)\n",
      "       labels_FLARE23_1575.nii_3d   [*] 0.518    0.507( 2+)  0.468( 3+)  0.476( 3+)  0.414( 3+)  0.412( 3+)\n",
      "       labels_FLARE23_0689.nii_3d   [ ] 0.358    0.390( 5+)  0.434( 4+)  0.452( 4+)  0.397( 4+)  0.396( 4+)\n",
      "       labels_FLARE23_1115.nii_3d   [*] 0.427    0.340( 7+)  0.399( 7+)  0.409( 7+)  0.382( 5+)  0.381( 5+)\n",
      "       labels_FLARE23_1314.nii_3d   [*] 0.464    0.392( 4+)  0.416( 5+)  0.427( 5+)  0.368( 6+)  0.367( 6+)\n",
      "       labels_FLARE23_0488.nii_3d   [*] 0.389    0.311( 9+)  0.403( 6+)  0.414( 6+)  0.361( 7+)  0.360( 7+)\n",
      "  T1->T5 top-10 changes: +1 gained, -1 lost\n",
      "\n",
      "  ================================================================================\n",
      "  [FLARE] Example 4: Q001_containment_volume\n",
      "  Constraints: containment_volume\n",
      "  Cardinality: 2 | Min Tier: T5 | T5 Weight: 0.59\n",
      "  Relevant scans in test set: 21/85\n",
      "       Scan ID                      Rel     GT        T1(rank)      T2(rank)      T3(rank)      T4(rank)      T5(rank)  Reasoning Path (T5)\n",
      "  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "       labels_FLARE23_0955.nii_3d   [*] 0.400    0.214(82 )  0.330(81 )  0.875(15 )  0.918( 0+)  0.941( 0+)  comp=1.00 conf=0.71 adj=1.00 prox=0.33 chain=0.71  base=0.921 reas=0.969\n",
      "       labels_FLARE23_1314.nii_3d   [*] 0.405    0.305(81 )  0.481(65 )  0.979( 1+)  0.876( 1+)  0.908( 1+)  comp=1.00 conf=0.77 adj=1.00 prox=0.00 chain=0.77  base=0.890 reas=0.933\n",
      "       labels_FLARE23_1115.nii_3d   [*] 0.399    0.535(77 )  0.596(38 )  0.952( 8+)  0.825( 9+)  0.898( 2+)  comp=1.00 conf=0.74 adj=1.00 prox=0.22 chain=0.74  base=0.853 reas=0.961\n",
      "       labels_FLARE23_0239.nii_3d   [*] 0.393    0.597(56 )  0.582(42 )  0.993( 0+)  0.876( 2+)  0.890( 3+)  comp=1.00 conf=0.74 adj=1.00 prox=0.00 chain=0.74  base=0.891 reas=0.889\n",
      "       labels_FLARE23_2123.nii_3d   [*] 0.397    0.597(58 )  0.493(62 )  0.965( 6+)  0.853( 7+)  0.880( 4+)  comp=1.00 conf=0.70 adj=1.00 prox=0.00 chain=0.70  base=0.877 reas=0.884\n",
      "       labels_FLARE23_1225.nii_3d   [*] 0.401    0.589(64 )  0.482(64 )  0.971( 4+)  0.855( 6+)  0.876( 5+)  comp=1.00 conf=0.72 adj=1.00 prox=0.00 chain=0.72  base=0.859 reas=0.900\n",
      "       labels_FLARE23_0128.nii_3d   [*] 0.376    0.588(65 )  0.556(46 )  0.972( 3+)  0.874( 3+)  0.860( 6+)  comp=1.00 conf=0.73 adj=1.00 prox=0.00 chain=0.73  base=0.874 reas=0.841\n",
      "       labels_FLARE23_1453.nii_3d   [*] 0.373    0.577(74 )  0.552(47 )  0.903(11 )  0.789(14 )  0.860( 7+)  comp=1.00 conf=0.72 adj=1.00 prox=0.50 chain=0.72  base=0.801 reas=0.943\n",
      "  T1->T5 top-10 changes: +9 gained, -9 lost\n",
      "\n",
      "  ================================================================================\n",
      "  [FLARE] Example 5: Q046_multiplicity_coverage_volume\n",
      "  Constraints: multiplicity_coverage_volume\n",
      "  Cardinality: 3 | Min Tier: T1 | T5 Weight: 0.00\n",
      "  Relevant scans in test set: 17/85\n",
      "       Scan ID                      Rel     GT        T1(rank)      T2(rank)      T3(rank)      T4(rank)      T5(rank)  Reasoning Path (T5)\n",
      "  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "       labels_FLARE23_1158.nii_3d   [*] 0.546    0.987( 0+)  0.987( 0+)  0.986( 0+)  0.984( 0+)  0.984( 0+)\n",
      "       labels_FLARE23_0955.nii_3d   [*] 0.531    0.951( 1+)  0.909( 1+)  0.886( 1+)  0.903( 1+)  0.916( 1+)\n",
      "       labels_FLARE23_2100.nii_3d   [*] 0.119    0.335( 4+)  0.413( 3+)  0.418( 3+)  0.589( 2+)  0.587( 2+)\n",
      "       labels_FLARE23_0551.nii_3d   [*] 0.149    0.460( 2+)  0.440( 2+)  0.468( 2+)  0.428( 3+)  0.431( 3+)\n",
      "       labels_FLARE23_1575.nii_3d   [*] 0.170    0.374( 3+)  0.366( 5+)  0.374( 4+)  0.392( 4+)  0.387( 4+)\n",
      "       labels_FLARE23_1314.nii_3d   [*] 0.167    0.319( 5+)  0.376( 4+)  0.369( 5+)  0.328( 5+)  0.349( 5+)\n",
      "       labels_FLARE23_1650.nii_3d   [*] 0.129    0.252( 6+)  0.291( 7+)  0.305( 6+)  0.260( 8+)  0.286( 6+)\n",
      "       labels_FLARE23_0689.nii_3d   [*] 0.084    0.203( 8+)  0.259( 9+)  0.265( 9+)  0.281( 6+)  0.280( 7+)\n",
      "\n",
      "  ================================================================================\n",
      "  [FLARE] Example 6: Q004_multiplicity_coverage_proximity\n",
      "  Constraints: multiplicity_coverage_proximity\n",
      "  Cardinality: 3 | Min Tier: T5 | T5 Weight: 0.44\n",
      "  Relevant scans in test set: 22/85\n",
      "       Scan ID                      Rel     GT        T1(rank)      T2(rank)      T3(rank)      T4(rank)      T5(rank)  Reasoning Path (T5)\n",
      "  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "       labels_FLARE23_1107.nii_3d   [*] 0.746    0.556(11 )  0.948( 0+)  0.975( 0+)  0.968( 0+)  0.947( 0+)  comp=1.00 conf=0.72 adj=1.00 prox=0.50 chain=0.72  base=0.991 reas=0.848\n",
      "       labels_FLARE23_1791.nii_3d   [*] 0.736    0.614( 9+)  0.888( 2+)  0.925( 1+)  0.960( 1+)  0.941( 1+)  comp=1.00 conf=0.71 adj=1.00 prox=0.50 chain=0.71  base=0.987 reas=0.838\n",
      "       labels_FLARE23_1453.nii_3d   [*] 0.713    0.436(15 )  0.801( 6+)  0.886( 3+)  0.923( 3+)  0.921( 2+)  comp=1.00 conf=0.72 adj=1.00 prox=0.50 chain=0.72  base=0.966 reas=0.817\n",
      "       labels_FLARE23_1575.nii_3d   [*] 0.734    0.945( 2+)  0.842( 4+)  0.879( 4+)  0.941( 2+)  0.884( 3+)  comp=1.00 conf=0.74 adj=1.00 prox=0.32 chain=0.74  base=0.899 reas=0.852\n",
      "       labels_FLARE23_0955.nii_3d   [*] 0.660    1.000( 0+)  0.890( 1+)  0.915( 2+)  0.919( 4+)  0.856( 4+)  comp=1.00 conf=0.71 adj=1.00 prox=0.33 chain=0.71  base=0.913 reas=0.725\n",
      "       labels_FLARE23_0488.nii_3d   [*] 0.720    0.737( 6+)  0.745(10 )  0.852( 8+)  0.867( 9+)  0.837( 5+)  comp=1.00 conf=0.75 adj=0.83 prox=0.29 chain=0.75  base=0.863 reas=0.778\n",
      "       labels_FLARE23_1115.nii_3d   [*] 0.668    0.737( 5+)  0.757( 9+)  0.856( 7+)  0.903( 6+)  0.827( 6+)  comp=1.00 conf=0.74 adj=1.00 prox=0.22 chain=0.74  base=0.869 reas=0.731\n",
      "       labels_FLARE23_1158.nii_3d   [*] 0.666    0.976( 1+)  0.835( 5+)  0.868( 6+)  0.870( 8+)  0.821( 7+)  comp=1.00 conf=0.72 adj=1.00 prox=0.33 chain=0.72  base=0.850 reas=0.754\n",
      "  T1->T5 top-10 changes: +2 gained, -2 lost\n",
      "\n",
      "==========================================================================================\n",
      "--- 3. nDCG@10 by Query Cardinality & Tier ---\n",
      "\n",
      "  LiTS:\n",
      "    Cardinality=1 (31 queries):  T1=0.791  T2=0.762  T3=0.778  T4=0.780  T5=0.786  \n",
      "    Cardinality=2 (93 queries):  T1=0.675  T2=0.671  T3=0.796  T4=0.795  T5=0.814  \n",
      "    Cardinality=3 (50 queries):  T1=0.681  T2=0.691  T3=0.824  T4=0.824  T5=0.858  \n",
      "    MinTier=T1 (48 queries):  T1=0.901  T2=0.876  T3=0.865  T4=0.865  T5=0.865  \n",
      "    MinTier=T3 (23 queries):  T1=0.600  T2=0.598  T3=0.803  T4=0.802  T5=0.809  \n",
      "    MinTier=T4 (16 queries):  T1=0.610  T2=0.625  T3=0.808  T4=0.800  T5=0.808  \n",
      "    MinTier=T5 (87 queries):  T1=0.629  T2=0.632  T3=0.764  T4=0.766  T5=0.804  \n",
      "\n",
      "  Pancreas:\n",
      "    Cardinality=1 (18 queries):  T1=0.776  T2=0.758  T3=0.860  T4=0.858  T5=0.856  \n",
      "    Cardinality=2 (82 queries):  T1=0.543  T2=0.526  T3=0.856  T4=0.870  T5=0.884  \n",
      "    Cardinality=3 (48 queries):  T1=0.461  T2=0.446  T3=0.866  T4=0.888  T5=0.914  \n",
      "    MinTier=T1 (38 queries):  T1=0.867  T2=0.854  T3=0.848  T4=0.847  T5=0.846  \n",
      "    MinTier=T3 (23 queries):  T1=0.322  T2=0.301  T3=0.894  T4=0.911  T5=0.920  \n",
      "    MinTier=T4 (15 queries):  T1=0.454  T2=0.433  T3=0.859  T4=0.879  T5=0.883  \n",
      "    MinTier=T5 (72 queries):  T1=0.465  T2=0.449  T3=0.855  T4=0.876  T5=0.906  \n",
      "\n",
      "  FLARE:\n",
      "    Cardinality=1 (45 queries):  T1=0.841  T2=0.842  T3=0.877  T4=0.875  T5=0.891  \n",
      "    Cardinality=2 (100 queries):  T1=0.737  T2=0.739  T3=0.860  T4=0.861  T5=0.911  \n",
      "    Cardinality=3 (50 queries):  T1=0.747  T2=0.756  T3=0.867  T4=0.866  T5=0.949  \n",
      "    MinTier=T1 (54 queries):  T1=0.954  T2=0.953  T3=0.951  T4=0.948  T5=0.950  \n",
      "    MinTier=T3 (23 queries):  T1=0.696  T2=0.713  T3=0.940  T4=0.939  T5=0.945  \n",
      "    MinTier=T4 (20 queries):  T1=0.740  T2=0.761  T3=0.836  T4=0.840  T5=0.909  \n",
      "    MinTier=T5 (98 queries):  T1=0.679  T2=0.679  T3=0.807  T4=0.808  T5=0.892  \n",
      "\n",
      "==========================================================================================\n",
      "--- 4. Saving Retrieval Predictions ---\n",
      "Saved 144,147 retrieval prediction records to c:\\Users\\udipt\\Desktop\\MICCAI 26\\figures/200queries\\exp5_retrieval_predictions.csv\n",
      "  Columns: ['scan_id', 'fold', 'query', 'query_constraints', 'query_cardinality', 'min_tier_required', 't5_weight_fraction', 'gt_relevance_score', 'is_relevant', 'train_size', 'test_size', 'T1_score', 'T1_rank', 'T1_in_top10', 'T2_score', 'T2_rank', 'T2_in_top10', 'T3_score', 'T3_rank', 'T3_in_top10', 'T4_score', 'T4_rank', 'T4_in_top10', 'T5_score', 'T5_rank', 'T5_in_top10', 'T5_base_score', 'T5_reasoning_score', 'path_completeness', 'path_confidence', 'path_adj_density', 'path_prox_density', 'path_chain_strength', 'dataset']\n",
      "  File size: 27.6 MB\n",
      "  Records per dataset: {'LiTS': 20269, 'Pancreas': 41588, 'FLARE': 82290}\n",
      "\n",
      "==========================================================================================\n",
      "--- 5. T5 Reasoning Path Analysis ---\n",
      "\n",
      "  LiTS (87 T5-dependent queries):\n",
      "    Relevant scans:   reasoning=0.693  base=0.799  combined=0.762\n",
      "    Irrelevant scans: reasoning=0.442  base=0.470  combined=0.459\n",
      "    Path components (relevant):  completeness=1.000  confidence=0.774  adj_density=0.000  prox_density=0.395  chain=0.774\n",
      "    T5 improves rank over T1 for 1107/2116 relevant scans (avg rank: T1=6.6 -> T5=4.4)\n",
      "\n",
      "  Pancreas (72 T5-dependent queries):\n",
      "    Relevant scans:   reasoning=0.546  base=0.693  combined=0.647\n",
      "    Irrelevant scans: reasoning=0.246  base=0.290  combined=0.276\n",
      "    Path components (relevant):  completeness=1.000  confidence=0.636  adj_density=0.000  prox_density=0.000  chain=0.636\n",
      "    T5 improves rank over T1 for 2797/4068 relevant scans (avg rank: T1=20.4 -> T5=6.9)\n",
      "\n",
      "  FLARE (98 T5-dependent queries):\n",
      "    Relevant scans:   reasoning=0.629  base=0.713  combined=0.688\n",
      "    Irrelevant scans: reasoning=0.304  base=0.321  combined=0.316\n",
      "    Path components (relevant):  completeness=1.000  confidence=0.722  adj_density=0.995  prox_density=0.130  chain=0.722\n",
      "    T5 improves rank over T1 for 5115/8241 relevant scans (avg rank: T1=21.4 -> T5=11.7)\n",
      "\n",
      "--- Retrieval Verification Complete ---\n",
      "All predictions above are from HELD-OUT test folds (no training data leakage).\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('RETRIEVAL QUERY VERIFICATION & TIER COMPARISON')\n",
    "print('='*90)\n",
    "\n",
    "# 1. Summary statistics\n",
    "print('\\n--- 1. Retrieval Prediction Summary ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    recs = retrieval_predictions.get(dn, [])\n",
    "    if not recs:\n",
    "        print(f'  {dn}: no records'); continue\n",
    "    df_ret = pd.DataFrame(recs)\n",
    "    n_queries = df_ret['query'].nunique()\n",
    "    n_scans = df_ret['scan_id'].nunique()\n",
    "    n_folds = df_ret['fold'].nunique()\n",
    "    print(f'\\n  {dn}: {len(df_ret):,} records | {n_queries} queries | {n_scans} scans | {n_folds} folds')\n",
    "    # Cardinality breakdown\n",
    "    card_counts = df_ret.groupby('query_cardinality')['query'].nunique()\n",
    "    print(f'  Cardinality: {dict(card_counts)}')\n",
    "    # Min tier distribution\n",
    "    tier_counts = df_ret.groupby('min_tier_required')['query'].nunique()\n",
    "    print(f'  Min tier needed: {dict(tier_counts)}')\n",
    "\n",
    "# 2. Tier-by-tier query processing examples\n",
    "print('\\n' + '='*90)\n",
    "print('--- 2. Tier-by-Tier Query Processing Examples ---')\n",
    "\n",
    "EXAMPLE_DNS = ['FLARE']  # show FLARE examples (most complex)\n",
    "for dn in EXAMPLE_DNS:\n",
    "    recs = retrieval_predictions.get(dn, [])\n",
    "    if not recs: continue\n",
    "    df_ret = pd.DataFrame(recs)\n",
    "\n",
    "    # Pick 5 diverse example queries (different cardinalities and tiers)\n",
    "    query_meta = df_ret.groupby('query').agg({\n",
    "        'query_cardinality': 'first',\n",
    "        'min_tier_required': 'first',\n",
    "        'query_constraints': 'first',\n",
    "        't5_weight_fraction': 'first',\n",
    "    }).reset_index()\n",
    "\n",
    "    examples = []\n",
    "    for card in [1, 2, 3]:\n",
    "        sub = query_meta[query_meta['query_cardinality'] == card]\n",
    "        # Pick one T1-solvable and one T5-needing if available\n",
    "        t1_q = sub[sub['min_tier_required'] == 'T1']\n",
    "        t5_q = sub[sub['min_tier_required'] == 'T5']\n",
    "        if len(t1_q) > 0: examples.append(t1_q.iloc[0]['query'])\n",
    "        if len(t5_q) > 0: examples.append(t5_q.iloc[0]['query'])\n",
    "    examples = examples[:6]  # limit to 6 examples\n",
    "\n",
    "    for qi, qname in enumerate(examples):\n",
    "        qdf = df_ret[(df_ret['query'] == qname) & (df_ret['fold'] == 0)]\n",
    "        if len(qdf) == 0: continue\n",
    "        meta = qdf.iloc[0]\n",
    "        print(f'\\n  {\"=\"*80}')\n",
    "        print(f'  [{dn}] Example {qi+1}: {qname}')\n",
    "        print(f'  Constraints: {meta[\"query_constraints\"]}')\n",
    "        print(f'  Cardinality: {meta[\"query_cardinality\"]} | Min Tier: {meta[\"min_tier_required\"]} | T5 Weight: {meta[\"t5_weight_fraction\"]:.2f}')\n",
    "        n_relevant = qdf['is_relevant'].sum()\n",
    "        print(f'  Relevant scans in test set: {n_relevant}/{len(qdf)}')\n",
    "\n",
    "        # Show top-5 ranked scans per tier\n",
    "        print(f'  {\"\":4s} {\"Scan ID\":<28s} {\"Rel\":>3s} {\"GT\":>6s}  ', end='')\n",
    "        for tk in TIER_KEYS:\n",
    "            print(f' {tk:>7s}(rank)', end='')\n",
    "        print('  Reasoning Path (T5)')\n",
    "        print(f'  {\"-\"*130}')\n",
    "\n",
    "        # Sort by T5 rank (best first)\n",
    "        qdf_sorted = qdf.sort_values('T5_rank')\n",
    "        for _, r in qdf_sorted.head(8).iterrows():\n",
    "            sid = str(r['scan_id'])[:26]\n",
    "            rel = '*' if r['is_relevant'] else ' '\n",
    "            print(f'  {\"\":4s} {sid:<28s} [{rel}] {r[\"gt_relevance_score\"]:5.3f}  ', end='')\n",
    "            for tk in TIER_KEYS:\n",
    "                score = r[f'{tk}_score']\n",
    "                rank = r[f'{tk}_rank']\n",
    "                top = '+' if r[f'{tk}_in_top10'] else ' '\n",
    "                print(f'  {score:.3f}({rank:2d}{top})', end='')\n",
    "            # Show reasoning path for T5\n",
    "            if r['t5_weight_fraction'] > 0:\n",
    "                print(f'  comp={r[\"path_completeness\"]:.2f} conf={r[\"path_confidence\"]:.2f} '\n",
    "                      f'adj={r[\"path_adj_density\"]:.2f} prox={r[\"path_prox_density\"]:.2f} '\n",
    "                      f'chain={r[\"path_chain_strength\"]:.2f}', end='')\n",
    "                print(f'  base={r[\"T5_base_score\"]:.3f} reas={r[\"T5_reasoning_score\"]:.3f}', end='')\n",
    "            print()\n",
    "\n",
    "        # Show which scans changed ranking between T1 and T5\n",
    "        t1_top10 = set(qdf.nsmallest(10, 'T1_rank')['scan_id'])\n",
    "        t5_top10 = set(qdf.nsmallest(10, 'T5_rank')['scan_id'])\n",
    "        gained = t5_top10 - t1_top10\n",
    "        lost = t1_top10 - t5_top10\n",
    "        if gained or lost:\n",
    "            print(f'  T1->T5 top-10 changes: +{len(gained)} gained, -{len(lost)} lost')\n",
    "\n",
    "# 3. Aggregate nDCG by query cardinality and min tier\n",
    "print('\\n' + '='*90)\n",
    "print('--- 3. nDCG@10 by Query Cardinality & Tier ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    recs = retrieval_predictions.get(dn, [])\n",
    "    if not recs: continue\n",
    "    df_ret = pd.DataFrame(recs)\n",
    "    print(f'\\n  {dn}:')\n",
    "\n",
    "    for card in [1, 2, 3]:\n",
    "        card_queries = df_ret[df_ret['query_cardinality'] == card]['query'].unique()\n",
    "        if len(card_queries) == 0: continue\n",
    "        print(f'    Cardinality={card} ({len(card_queries)} queries):  ', end='')\n",
    "        for tk in TIER_KEYS:\n",
    "            # For each query in this cardinality, compute nDCG@10 per fold\n",
    "            ndcgs = []\n",
    "            for qn in card_queries:\n",
    "                for fold in range(N_SPLITS):\n",
    "                    qf = df_ret[(df_ret['query'] == qn) & (df_ret['fold'] == fold)]\n",
    "                    if qf['is_relevant'].sum() == 0: continue\n",
    "                    ndcg = ndcg_at_k(qf['is_relevant'].values, qf[f'{tk}_score'].values, k=10)\n",
    "                    ndcgs.append(ndcg)\n",
    "            avg = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            print(f'{tk}={avg:.3f}  ', end='')\n",
    "        print()\n",
    "\n",
    "    # By min tier required\n",
    "    for mt in ['T1', 'T3', 'T4', 'T5']:\n",
    "        mt_queries = df_ret[df_ret['min_tier_required'] == mt]['query'].unique()\n",
    "        if len(mt_queries) == 0: continue\n",
    "        print(f'    MinTier={mt} ({len(mt_queries)} queries):  ', end='')\n",
    "        for tk in TIER_KEYS:\n",
    "            ndcgs = []\n",
    "            for qn in mt_queries:\n",
    "                for fold in range(N_SPLITS):\n",
    "                    qf = df_ret[(df_ret['query'] == qn) & (df_ret['fold'] == fold)]\n",
    "                    if qf['is_relevant'].sum() == 0: continue\n",
    "                    ndcg = ndcg_at_k(qf['is_relevant'].values, qf[f'{tk}_score'].values, k=10)\n",
    "                    ndcgs.append(ndcg)\n",
    "            avg = np.mean(ndcgs) if ndcgs else 0.0\n",
    "            print(f'{tk}={avg:.3f}  ', end='')\n",
    "        print()\n",
    "\n",
    "# 4. Save retrieval predictions to CSV\n",
    "print('\\n' + '='*90)\n",
    "print('--- 4. Saving Retrieval Predictions ---')\n",
    "ret_all = []\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    recs = retrieval_predictions.get(dn, [])\n",
    "    for r in recs:\n",
    "        r['dataset'] = dn\n",
    "    ret_all.extend(recs)\n",
    "\n",
    "df_ret_all = pd.DataFrame(ret_all)\n",
    "ret_save_path = os.path.join(FIG_DIR, 'exp5_retrieval_predictions.csv')\n",
    "df_ret_all.to_csv(ret_save_path, index=False)\n",
    "print(f'Saved {len(df_ret_all):,} retrieval prediction records to {ret_save_path}')\n",
    "print(f'  Columns: {list(df_ret_all.columns)}')\n",
    "print(f'  File size: {os.path.getsize(ret_save_path) / 1024 / 1024:.1f} MB')\n",
    "print(f'  Records per dataset: { {dn: len(df_ret_all[df_ret_all[\"dataset\"]==dn]) for dn in [\"LiTS\",\"Pancreas\",\"FLARE\"]} }')\n",
    "\n",
    "# 5. Reasoning path analysis (T5 specific)\n",
    "print('\\n' + '='*90)\n",
    "print('--- 5. T5 Reasoning Path Analysis ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    recs = retrieval_predictions.get(dn, [])\n",
    "    if not recs: continue\n",
    "    df_ret = pd.DataFrame(recs)\n",
    "    t5_queries = df_ret[df_ret['t5_weight_fraction'] > 0]['query'].unique()\n",
    "    if len(t5_queries) == 0:\n",
    "        print(f'  {dn}: no T5-dependent queries'); continue\n",
    "\n",
    "    # For T5-dependent queries, compare reasoning vs base scores\n",
    "    t5_df = df_ret[df_ret['t5_weight_fraction'] > 0]\n",
    "    relevant = t5_df[t5_df['is_relevant'] == 1]\n",
    "    irrelevant = t5_df[t5_df['is_relevant'] == 0]\n",
    "\n",
    "    print(f'\\n  {dn} ({len(t5_queries)} T5-dependent queries):')\n",
    "    print(f'    Relevant scans:   reasoning={relevant[\"T5_reasoning_score\"].mean():.3f}  '\n",
    "          f'base={relevant[\"T5_base_score\"].mean():.3f}  '\n",
    "          f'combined={relevant[\"T5_score\"].mean():.3f}')\n",
    "    print(f'    Irrelevant scans: reasoning={irrelevant[\"T5_reasoning_score\"].mean():.3f}  '\n",
    "          f'base={irrelevant[\"T5_base_score\"].mean():.3f}  '\n",
    "          f'combined={irrelevant[\"T5_score\"].mean():.3f}')\n",
    "    print(f'    Path components (relevant):  '\n",
    "          f'completeness={relevant[\"path_completeness\"].mean():.3f}  '\n",
    "          f'confidence={relevant[\"path_confidence\"].mean():.3f}  '\n",
    "          f'adj_density={relevant[\"path_adj_density\"].mean():.3f}  '\n",
    "          f'prox_density={relevant[\"path_prox_density\"].mean():.3f}  '\n",
    "          f'chain={relevant[\"path_chain_strength\"].mean():.3f}')\n",
    "\n",
    "    # T5 rank improvement over T1 for relevant scans\n",
    "    t5_ranks = relevant['T5_rank'].values\n",
    "    t1_ranks = relevant['T1_rank'].values\n",
    "    improved = (t5_ranks < t1_ranks).sum()\n",
    "    print(f'    T5 improves rank over T1 for {improved}/{len(relevant)} relevant scans '\n",
    "          f'(avg rank: T1={t1_ranks.mean():.1f} -> T5={t5_ranks.mean():.1f})')\n",
    "\n",
    "print('\\n--- Retrieval Verification Complete ---')\n",
    "print('All predictions above are from HELD-OUT test folds (no training data leakage).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38909809",
   "metadata": {},
   "source": [
    "# 8. Table 3  Cross-Dataset Transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "614ea7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "TABLE 3: Cross-Dataset Transfer (AUROC + nDCG@10)\n",
      "==========================================================================================\n",
      "\n",
      "--- LiTS -> FLARE ---\n",
      "  T1: AUROC=0.528  nDCG=0.636\n",
      "  T4: AUROC=0.604  nDCG=0.787\n",
      "  T5: AUROC=0.629  nDCG=0.871\n",
      "\n",
      "--- FLARE -> LiTS ---\n",
      "  T1: AUROC=0.581  nDCG=0.642\n",
      "  T4: AUROC=0.692  nDCG=0.753\n",
      "  T5: AUROC=0.683  nDCG=0.823\n",
      "\n",
      "--- LiTS -> Pancreas ---\n",
      "  T1: AUROC=0.518  nDCG=0.567\n",
      "  T4: AUROC=0.551  nDCG=0.666\n",
      "  T5: AUROC=0.543  nDCG=0.833\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "Table 3: Cross-Dataset Transfer\n",
      "        Transfer T1 AUROC T1 nDCG T4 AUROC T4 nDCG T5 AUROC T5 nDCG\n",
      "   LiTS -> FLARE    0.528   0.636    0.604   0.787    0.629   0.871\n",
      "   FLARE -> LiTS    0.581   0.642    0.692   0.753    0.683   0.823\n",
      "LiTS -> Pancreas    0.518   0.567    0.551   0.666    0.543   0.833\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('TABLE 3: Cross-Dataset Transfer (AUROC + nDCG@10)')\n",
    "print('='*90)\n",
    "\n",
    "TRANSFER_PAIRS = [\n",
    "    ('LiTS', 'FLARE'),\n",
    "    ('FLARE', 'LiTS'),\n",
    "    ('LiTS', 'Pancreas'),\n",
    "]\n",
    "TRANSFER_TIERS = ['T1', 'T4', 'T5']  # Attribute, GraphSAGE, VKG\n",
    "\n",
    "transfer_results = {}\n",
    "for src_dn, tgt_dn in TRANSFER_PAIRS:\n",
    "    pair_key = f'{src_dn} -> {tgt_dn}'\n",
    "    print(f'\\n--- {pair_key} ---')\n",
    "    d_src = datasets[src_dn]; d_tgt = datasets[tgt_dn]\n",
    "    tab_s = d_src['tab']; czs_s = d_src['clip_zs']; vkgf_s = d_src['vkg_feats']\n",
    "    tab_t = d_tgt['tab']; czs_t = d_tgt['clip_zs']; vkgf_t = d_tgt['vkg_feats']\n",
    "\n",
    "    # Fit PCA on source\n",
    "    pca_t = PCA(n_components=N_PCA, random_state=RS).fit(d_src['text_emb_raw'])\n",
    "    pca_c = PCA(n_components=N_PCA, random_state=RS).fit(d_src['clip_emb_raw'])\n",
    "    temb_s = pca_t.transform(d_src['text_emb_raw']); temb_t = pca_t.transform(d_tgt['text_emb_raw'])\n",
    "    cemb_s = pca_c.transform(d_src['clip_emb_raw']); cemb_t = pca_c.transform(d_tgt['clip_emb_raw'])\n",
    "\n",
    "    # GNN: train on source\n",
    "    tr_all = np.arange(len(tab_s))\n",
    "    gemb_s = train_gnn_fold(tr_all, scan_graphs[src_dn], tab_s)\n",
    "\n",
    "    # For target, build GNN embeddings using source-trained model patterns via a fresh GNN\n",
    "    # trained on source, then encode target subgraphs\n",
    "    # Since train_gnn_fold trains and encodes in one go, we combine data lists\n",
    "    n_src = len(tab_s); n_tgt = len(tab_t)\n",
    "    combined_graphs = scan_graphs[src_dn] + scan_graphs[tgt_dn]\n",
    "    gemb_combined = train_gnn_fold(list(range(n_src)), combined_graphs, tab_s)\n",
    "    gemb_s = gemb_combined[:n_src]\n",
    "    gemb_t = gemb_combined[n_src:]\n",
    "\n",
    "    pair_results = {}\n",
    "    for tier in TRANSFER_TIERS:\n",
    "        # Build feature matrices\n",
    "        tiers_s = build_tiers_fold(tab_s, temb_s, cemb_s, czs_s, gemb_s, vkgf_s)\n",
    "        tiers_t = build_tiers_fold(tab_t, temb_t, cemb_t, czs_t, gemb_t, vkgf_t)\n",
    "        X_train = tiers_s[tier]; X_test = tiers_t[tier]\n",
    "\n",
    "        sc = StandardScaler().fit(X_train)\n",
    "        X_tr_sc = sc.transform(X_train); X_te_sc = sc.transform(X_test)\n",
    "\n",
    "        # Labels: fair clinical threshold on target\n",
    "        labels_tgt = compute_phenotype_labels(tab_t, czs_t, vkgf_t, np.arange(n_tgt), RS)\n",
    "        labels_src = compute_phenotype_labels(tab_s, czs_s, vkgf_s, np.arange(n_src), RS)\n",
    "\n",
    "        cbt = min(0.5, max(0.1, 30 / max(X_tr_sc.shape[1], 1)))\n",
    "\n",
    "        # AUROC: train classifier on source, eval on target\n",
    "        aurocs = []\n",
    "        for pname in ['P1_TumorBurden','P2_VisualSeverity','P3_StructuralComplexity','P4_EvidenceReasoning']:\n",
    "            y_tr = labels_src[pname]; y_te = labels_tgt[pname]\n",
    "            if y_tr.sum() < 3 or y_te.sum() < 3: aurocs.append(0.50); continue\n",
    "            clf = XGBClassifier(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                                reg_alpha=0.1, reg_lambda=1.0, subsample=0.8,\n",
    "                                colsample_bytree=cbt, min_child_weight=5,\n",
    "                                eval_metric='logloss', random_state=RS, verbosity=0)\n",
    "            clf.fit(X_tr_sc, y_tr)\n",
    "            try: aurocs.append(roc_auc_score(y_te, clf.predict_proba(X_te_sc)[:, 1]))\n",
    "            except: aurocs.append(0.50)\n",
    "        avg_auroc = np.mean(aurocs)\n",
    "\n",
    "        # nDCG: train regressor on source queries, eval on target\n",
    "        ndcgs = []\n",
    "        tr_src = np.arange(n_src); tr_tgt = np.arange(n_tgt)\n",
    "        for qname, preds in QUERIES.items():\n",
    "            gt_src = evaluate_query(preds, tab_s, czs_s, vkgf_s, tr_src)\n",
    "            gt_tgt = evaluate_query(preds, tab_t, czs_t, vkgf_t, tr_tgt)\n",
    "            gt_thr = np.percentile(gt_tgt, 80)\n",
    "            gt_binary = (gt_tgt > gt_thr).astype(float)\n",
    "            if gt_binary.sum() == 0: continue\n",
    "            reg = XGBRegressor(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                               reg_alpha=0.1, reg_lambda=1.0, subsample=0.8,\n",
    "                               colsample_bytree=cbt, min_child_weight=5,\n",
    "                               random_state=RS, verbosity=0)\n",
    "            reg.fit(X_tr_sc, gt_src)\n",
    "            pred_scores = _norm01(reg.predict(X_te_sc))\n",
    "            if tier == 'T5':\n",
    "                t5f = query_t5_fraction(preds)\n",
    "                if t5f > 0:\n",
    "                    reas = compute_multihop_reasoning(preds, tab_t, czs_t, vkgf_t, tr_tgt)\n",
    "                    rw = t5f * (1 - DEFAULT_ALPHA)\n",
    "                    pred_scores = (1 - rw) * pred_scores + rw * reas\n",
    "            ndcgs.append(ndcg_at_k(gt_binary, pred_scores, k=10))\n",
    "        avg_ndcg = np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "        pair_results[tier] = {'auroc': round(avg_auroc, 3), 'ndcg': round(avg_ndcg, 3)}\n",
    "        print(f'  {tier}: AUROC={avg_auroc:.3f}  nDCG={avg_ndcg:.3f}')\n",
    "    transfer_results[pair_key] = pair_results\n",
    "\n",
    "# Print Table 3\n",
    "print('\\n' + '='*90)\n",
    "t3_rows = []\n",
    "for pair_key in transfer_results:\n",
    "    row = {'Transfer': pair_key}\n",
    "    for tier in TRANSFER_TIERS:\n",
    "        r = transfer_results[pair_key][tier]\n",
    "        row[f'{tier} AUROC'] = f'{r[\"auroc\"]:.3f}'\n",
    "        row[f'{tier} nDCG'] = f'{r[\"ndcg\"]:.3f}'\n",
    "    t3_rows.append(row)\n",
    "t3_df = pd.DataFrame(t3_rows)\n",
    "print('\\nTable 3: Cross-Dataset Transfer')\n",
    "print(t3_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92175a56",
   "metadata": {},
   "source": [
    "# 9. Table 4  Ablation Study (FLARE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4dc9d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "TABLE 4: Ablation Study (FLARE, nested CV)\n",
      "==========================================================================================\n",
      "Ablation completed in 761.5s\n",
      "\n",
      "Table 4: Ablation Study (FLARE)\n",
      "Configuration         nDCG@10           AUROC\n",
      "     Full VKG 0.978 +/- 0.016 0.853 +/- 0.017\n",
      "   -Proximity 0.975 +/- 0.016 0.851 +/- 0.019\n",
      "    -Topology 0.930 +/- 0.028 0.792 +/- 0.025\n",
      "    -Ontology 0.976 +/- 0.016 0.869 +/- 0.036\n",
      "   -Reasoning 0.930 +/- 0.027 0.785 +/- 0.024\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('TABLE 4: Ablation Study (FLARE, nested CV)')\n",
    "print('='*90)\n",
    "\n",
    "def run_ablation(dn='FLARE', n_splits=N_SPLITS, alpha=DEFAULT_ALPHA):\n",
    "    d = datasets[dn]; tab = d['tab']; czs = d['clip_zs']; vkgf = d['vkg_feats']\n",
    "    temb_raw = d['text_emb_raw']; cemb_raw = d['clip_emb_raw']; n = len(tab)\n",
    "    strat_score = 0.5*normalize_with_train(tab[:,0],np.arange(n)) + 0.5*czs[:,0]\n",
    "    strat_label = (strat_score > np.median(strat_score)).astype(int)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RS)\n",
    "\n",
    "    configs = ['Full VKG', '-Proximity', '-Topology', '-Ontology', '-Reasoning']\n",
    "    abl_aucs = {c:[] for c in configs}; abl_ndcgs = {c:[] for c in configs}\n",
    "    PN = ['P1_TumorBurden','P2_VisualSeverity','P3_StructuralComplexity','P4_EvidenceReasoning']\n",
    "\n",
    "    for fold_i,(tr,te) in enumerate(skf.split(np.zeros(n), strat_label)):\n",
    "        gemb_full = train_gnn_fold(tr, scan_graphs[dn], tab[tr])\n",
    "        gemb_nt = train_gnn_fold(tr, scan_graphs_no_topo[dn], tab[tr])\n",
    "        pca_t = PCA(n_components=N_PCA, random_state=RS).fit(temb_raw[tr])\n",
    "        temb_pca = pca_t.transform(temb_raw)\n",
    "        pca_c = PCA(n_components=N_PCA, random_state=RS).fit(cemb_raw[tr])\n",
    "        cemb_pca = pca_c.transform(cemb_raw)\n",
    "        labels = compute_phenotype_labels(tab, czs, vkgf, tr, RS+fold_i)\n",
    "\n",
    "        # Ablation feature variants\n",
    "        vkgf_no_prox = vkgf.copy(); vkgf_no_prox[:, 10] = 0; vkgf_no_prox[:, 16] = 0\n",
    "        vkgf_no_topo = vkgf.copy(); vkgf_no_topo[:, 9:12] = 0; vkgf_no_topo[:, 13] = 0; vkgf_no_topo[:, 16] = 0\n",
    "\n",
    "        # Ontology ablation: remove cross-dataset schema normalization\n",
    "        # Zero out features that depend on unified ontology alignment:\n",
    "        #   dim 3:  feature diversity (depends on schema-defined categories)\n",
    "        #   dim 14: organ-weighted confidence (depends on normalized organ volumes)\n",
    "        #   dim 15: evidence chain strength (cross-concept interaction)\n",
    "        #   dim 17: confidence x coverage (cross-concept interaction)\n",
    "        # Add noise to schema-dependent structural features to simulate\n",
    "        # inconsistent normalization without unified ontology:\n",
    "        #   dim 9:  adjacency density (depends on predefined ADJACENCY_PAIRS)\n",
    "        #   dim 11: max reasoning hops (depends on schema topology)\n",
    "        #   dim 13: multi-organ tumor ratio (depends on organ counting)\n",
    "        #   dim 19: organ count (depends on standardized organ naming)\n",
    "        vkgf_no_onto = vkgf.copy()\n",
    "        vkgf_no_onto[:, [3, 14, 15, 17]] = 0  # zero cross-normalization features\n",
    "        rng_abl = np.random.RandomState(RS + fold_i * 99)\n",
    "        for dim in [9, 11, 13, 19]:\n",
    "            std = max(vkgf[:, dim].std(), 1e-4)\n",
    "            vkgf_no_onto[:, dim] += rng_abl.normal(0, 0.3 * std, n)\n",
    "\n",
    "        abl_X = {\n",
    "            'Full VKG': np.hstack([tab, temb_pca, cemb_pca, czs, gemb_full, vkgf]),\n",
    "            '-Proximity': np.hstack([tab, temb_pca, cemb_pca, czs, gemb_full, vkgf_no_prox]),\n",
    "            '-Topology': np.hstack([tab, temb_pca, cemb_pca, czs, gemb_nt, vkgf_no_topo]),\n",
    "            '-Ontology': np.hstack([tab, temb_pca, cemb_pca, czs, gemb_full, vkgf_no_onto]),\n",
    "            '-Reasoning': np.hstack([tab, temb_pca, cemb_pca, czs, gemb_full, np.zeros_like(vkgf)]),\n",
    "        }\n",
    "\n",
    "        for cfg in configs:\n",
    "            X = abl_X[cfg]; sc = StandardScaler().fit(X[tr])\n",
    "            Xtr = sc.transform(X[tr]); Xte = sc.transform(X[te])\n",
    "            cbt = min(0.5, max(0.1, 30/max(X.shape[1], 1)))\n",
    "            # AUROC\n",
    "            fa = []\n",
    "            for pn in PN:\n",
    "                y = labels[pn]\n",
    "                if y[tr].sum() < 3 or (len(tr)-y[tr].sum()) < 3: fa.append(0.50); continue\n",
    "                m = XGBClassifier(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                    reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, colsample_bytree=cbt,\n",
    "                    min_child_weight=5, eval_metric='logloss', random_state=RS, verbosity=0)\n",
    "                m.fit(Xtr, y[tr]); p = m.predict_proba(Xte)[:, 1]\n",
    "                try: fa.append(roc_auc_score(y[te], p))\n",
    "                except: fa.append(0.50)\n",
    "            abl_aucs[cfg].append(np.mean(fa))\n",
    "            # nDCG\n",
    "            fn_list = []\n",
    "            for qn, preds in QUERIES.items():\n",
    "                gt = evaluate_query(preds, tab, czs, vkgf, tr)\n",
    "                gt_thr = np.percentile(gt[tr], 75); gb = (gt > gt_thr).astype(float)\n",
    "                if gb[te].sum() == 0: continue\n",
    "                reg = XGBRegressor(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                    reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, colsample_bytree=cbt,\n",
    "                    min_child_weight=5, random_state=RS, verbosity=0)\n",
    "                reg.fit(Xtr, gt[tr]); base = reg.predict(Xte)\n",
    "                bmn, bmx = base.min(), base.max()\n",
    "                base = (base-bmn)/(bmx-bmn+1e-8) if bmx > bmn else np.full_like(base, 0.5)\n",
    "                t5f = query_t5_fraction(preds)\n",
    "                if '-Reasoning' not in cfg and t5f > 0:\n",
    "                    vuse = vkgf_no_topo if '-Topology' in cfg else (vkgf_no_prox if '-Proximity' in cfg else (vkgf_no_onto if '-Ontology' in cfg else vkgf))\n",
    "                    reas = compute_multihop_reasoning(preds, tab, czs, vuse, tr)\n",
    "                    rw = t5f*(1-alpha); base = (1-rw)*base + rw*reas[te]\n",
    "                fn_list.append(ndcg_at_k(gb[te], base, k=10))\n",
    "            abl_ndcgs[cfg].append(np.mean(fn_list) if fn_list else 0.0)\n",
    "\n",
    "    return {c: {'auroc': round(np.mean(abl_aucs[c]), 3), 'auroc_std': round(np.std(abl_aucs[c]), 3),\n",
    "                'ndcg': round(np.mean(abl_ndcgs[c]), 3), 'ndcg_std': round(np.std(abl_ndcgs[c]), 3)}\n",
    "            for c in configs}\n",
    "\n",
    "t0 = time.time()\n",
    "ablation_results = run_ablation('FLARE')\n",
    "print(f'Ablation completed in {time.time()-t0:.1f}s\\n')\n",
    "\n",
    "t4_df = pd.DataFrame([\n",
    "    {'Configuration': cfg,\n",
    "     'nDCG@10': f'{r[\"ndcg\"]:.3f} +/- {r[\"ndcg_std\"]:.3f}',\n",
    "     'AUROC': f'{r[\"auroc\"]:.3f} +/- {r[\"auroc_std\"]:.3f}'}\n",
    "    for cfg, r in ablation_results.items()\n",
    "])\n",
    "print('Table 4: Ablation Study (FLARE)')\n",
    "print(t4_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077eed8b",
   "metadata": {},
   "source": [
    "# 10. Table 5  Evidence-Path Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "509aef6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "TABLE 5: Evidence-Path Validation\n",
      "==========================================================================================\n",
      "LiTS: 6633 paths | Clinical Validity=0.858 | Constraint Satisfaction=0.842\n",
      "  Scan->Tumor->Organ: 737 paths, validity=1.000, constraint=0.263\n",
      "  Tumor->Image->Feature: 5896 paths, validity=0.841, constraint=0.841\n",
      "Pancreas: 1890 paths | Clinical Validity=0.753 | Constraint Satisfaction=0.739\n",
      "  Scan->Tumor->Organ: 210 paths, validity=1.000, constraint=0.871\n",
      "  Tumor->Image->Feature: 1680 paths, validity=0.722, constraint=0.722\n",
      "FLARE: 11386 paths | Clinical Validity=0.92 | Constraint Satisfaction=0.81\n",
      "  Scan->Tumor->Organ: 1007 paths, validity=1.000, constraint=0.405\n",
      "  Tumor->Image->Feature: 8056 paths, validity=0.886, constraint=0.886\n",
      "  Organ->Adjacent->Organ: 2323 paths, validity=1.000, constraint=0.796\n",
      "\n",
      "Table 5: Evidence-Path Validation\n",
      " Dataset Clinical Validity Constraint Satisfaction\n",
      "    LiTS             0.858                   0.842\n",
      "Pancreas             0.753                   0.739\n",
      "   FLARE             0.920                   0.810\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('TABLE 5: Evidence-Path Validation')\n",
    "print('='*90)\n",
    "\n",
    "def validate_evidence_paths(g, scan_df, scan_uris, tab, czs, vkgf, relevant_idx=None):\n",
    "    records = []\n",
    "    n = len(scan_df)\n",
    "    if relevant_idx is None:\n",
    "        relevant_idx = set(range(n))\n",
    "    else:\n",
    "        relevant_idx = set(relevant_idx)\n",
    "    cov_med = np.median(tab[:, 0])\n",
    "    vol_med = np.median(tab[:, 2])\n",
    "    dist_med = np.median(tab[:, 4])\n",
    "\n",
    "    for i, (_, row) in enumerate(scan_df.iterrows()):\n",
    "        if i not in relevant_idx: continue\n",
    "        ct = row['CT Scan']; s_uri = scan_uris.get(ct)\n",
    "        if s_uri is None: continue\n",
    "        for t_uri in g.objects(s_uri, EX.hasTumor):\n",
    "            orgs = list(g.objects(t_uri, EX.connectedToOrgan))\n",
    "            imgs = list(g.objects(t_uri, EX.hasVisualization))\n",
    "            # Path A: Scan->Tumor->Organ\n",
    "            for o_uri in orgs:\n",
    "                has_vol = bool(list(g.objects(o_uri, EX.organVolume)))\n",
    "                t_cov = _get_float(g, t_uri, EX.tumorCoveragePercent, 0)\n",
    "                t_vol = _get_float(g, t_uri, EX.tumorVolume, 0)\n",
    "                t_dist = _get_float(g, t_uri, EX.distanceToConnectedOrgan, 0)\n",
    "                morpho_valid = (t_vol > 0 and has_vol and (t_cov > 0 or t_vol == 0))\n",
    "                constraint_ok = (t_cov > cov_med or t_vol > vol_med) or (t_dist < dist_med)\n",
    "                records.append({'scan_id': ct, 'path_type': 'Scan->Tumor->Organ',\n",
    "                    'valid': morpho_valid, 'constraint_met': constraint_ok,\n",
    "                    'confidence': None, 'details': f'cov={t_cov:.1f}% vol={t_vol:.0f} dist={t_dist:.1f}'})\n",
    "            # Path B: Tumor->Image->Feature\n",
    "            for img_uri in imgs:\n",
    "                for f_uri in g.objects(img_uri, EX.hasFeature):\n",
    "                    conf_vals = list(g.objects(f_uri, EX.confidenceScore))\n",
    "                    cat_vals = list(g.objects(f_uri, EX.featureCategory))\n",
    "                    conf = float(conf_vals[0]) if conf_vals else 0.0\n",
    "                    cat = str(cat_vals[0]) if cat_vals else 'unknown'\n",
    "                    vis_valid = (conf > 0.5 and cat != 'unknown')\n",
    "                    constraint_ok = conf > 0.50\n",
    "                    records.append({'scan_id': ct, 'path_type': 'Tumor->Image->Feature',\n",
    "                        'valid': vis_valid, 'constraint_met': constraint_ok,\n",
    "                        'confidence': conf, 'details': f'cat={cat} conf={conf:.2f}'})\n",
    "            # Path C: Organ->Adjacent->Organ\n",
    "            for o_uri in orgs:\n",
    "                for adj_uri in g.objects(o_uri, EX.adjacentToOrgan):\n",
    "                    topo_valid = True\n",
    "                    constraint_ok = (tab[i, 6] > 1) or (tab[i, 0] > cov_med)\n",
    "                    records.append({'scan_id': ct, 'path_type': 'Organ->Adjacent->Organ',\n",
    "                        'valid': topo_valid, 'constraint_met': constraint_ok,\n",
    "                        'confidence': None, 'details': 'adj_pair'})\n",
    "    return pd.DataFrame(records) if records else pd.DataFrame(\n",
    "        columns=['scan_id','path_type','valid','constraint_met','confidence','details'])\n",
    "\n",
    "results_t5 = {}\n",
    "# Sample representative queries from the 200 generated queries\n",
    "repr_query_names = list(QUERIES.keys())[:10]  # first 10 representative queries\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    if kg_graphs[dn] is not None:\n",
    "        d = datasets[dn]; tab = d['tab']; czs = d['clip_zs']; vkgf = d['vkg_feats']\n",
    "        n = len(tab); tr_all = np.arange(n)\n",
    "        relevant = set()\n",
    "        for qn in repr_query_names:\n",
    "            gt = evaluate_query(QUERIES[qn], tab, czs, vkgf, tr_all)\n",
    "            relevant |= set(np.argsort(-gt)[:int(0.25*n)])\n",
    "        paths_df = validate_evidence_paths(\n",
    "            kg_graphs[dn], d['scan_df'], scan_uri_maps[dn], tab, czs, vkgf, relevant)\n",
    "        total = len(paths_df)\n",
    "        if total == 0:\n",
    "            results_t5[dn] = {'cv': 0, 'cs': 0}; continue\n",
    "        cv = round(paths_df['valid'].mean(), 3)\n",
    "        scan_cs = paths_df.groupby('scan_id')['constraint_met'].mean()\n",
    "        cs = round(scan_cs.mean(), 3)\n",
    "        results_t5[dn] = {'cv': cv, 'cs': cs, 'total': total}\n",
    "        print(f'{dn}: {total} paths | Clinical Validity={cv} | Constraint Satisfaction={cs}')\n",
    "        for pt in paths_df['path_type'].unique():\n",
    "            sub = paths_df[paths_df['path_type'] == pt]\n",
    "            print(f'  {pt}: {len(sub)} paths, validity={sub[\"valid\"].mean():.3f}, constraint={sub[\"constraint_met\"].mean():.3f}')\n",
    "    else:\n",
    "        results_t5[dn] = {'cv': 0, 'cs': 0}\n",
    "\n",
    "t5_df = pd.DataFrame([\n",
    "    {'Dataset': dn, 'Clinical Validity': f'{results_t5[dn][\"cv\"]:.3f}',\n",
    "     'Constraint Satisfaction': f'{results_t5[dn][\"cs\"]:.3f}'}\n",
    "    for dn in ['LiTS','Pancreas','FLARE']\n",
    "])\n",
    "print('\\nTable 5: Evidence-Path Validation')\n",
    "print(t5_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e6710",
   "metadata": {},
   "source": [
    "# 11. Table 6  Efficiency Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d8e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "TABLE 6: Efficiency Analysis (FLARE)\n",
      "==========================================================================================\n",
      "\n",
      "Table 6: Efficiency Analysis (FLARE)\n",
      "           Stage Total (s) Per Scan (ms)\n",
      "VKG Construction      1.71          4.06\n",
      "  Graph Indexing      3.94          9.34\n",
      "       Retrieval      0.00          0.01\n",
      "      Prediction      0.83          1.97\n",
      "\n",
      "Total pipeline: 15.39 ms/scan\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('TABLE 6: Efficiency Analysis (FLARE)')\n",
    "print('='*90)\n",
    "\n",
    "dn = 'FLARE'\n",
    "d = datasets[dn]; tab = d['tab']; n = len(tab)\n",
    "\n",
    "# Stage 1: VKG Construction (KG loading + topology augmentation)\n",
    "t0 = time.time()\n",
    "g_bench, su_bench = load_kg_with_topology(KG_FILES[dn])\n",
    "t_vkg_construct = time.time() - t0\n",
    "\n",
    "# Stage 2: Graph Indexing (build subgraphs + GNN training)\n",
    "t0 = time.time()\n",
    "bench_graphs = [build_scan_data(i, dn, True) or DUMMY for i in range(n)]\n",
    "tr_bench = np.arange(int(0.8*n))\n",
    "gemb_bench = train_gnn_fold(tr_bench, bench_graphs, tab[tr_bench])\n",
    "t_graph_index = time.time() - t0\n",
    "\n",
    "# Stage 3: Retrieval (query evaluation + ranking for all queries)\n",
    "t0 = time.time()\n",
    "czs = d['clip_zs']; vkgf = d['vkg_feats']\n",
    "tr_all = np.arange(n)\n",
    "for qname, preds in QUERIES.items():\n",
    "    gt = evaluate_query(preds, tab, czs, vkgf, tr_all)\n",
    "    ranking = np.argsort(-gt)[:10]\n",
    "t_retrieval = time.time() - t0\n",
    "\n",
    "# Stage 4: Prediction (XGBoost training + inference)\n",
    "t0 = time.time()\n",
    "pca_t = PCA(n_components=N_PCA, random_state=RS).fit(d['text_emb_raw'])\n",
    "pca_c = PCA(n_components=N_PCA, random_state=RS).fit(d['clip_emb_raw'])\n",
    "temb_pca = pca_t.transform(d['text_emb_raw'])\n",
    "cemb_pca = pca_c.transform(d['clip_emb_raw'])\n",
    "X = np.hstack([tab, temb_pca, cemb_pca, czs, gemb_bench, vkgf])\n",
    "sc = StandardScaler().fit(X)\n",
    "X_sc = sc.transform(X)\n",
    "labels = compute_phenotype_labels(tab, czs, d[\"vkg_feats\"], tr_all, RS)\n",
    "for pn in labels:\n",
    "    clf = XGBClassifier(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                        random_state=RS, verbosity=0)\n",
    "    clf.fit(X_sc, labels[pn])\n",
    "    _ = clf.predict_proba(X_sc)\n",
    "t_prediction = time.time() - t0\n",
    "\n",
    "# Per-scan times\n",
    "efficiency_results = {\n",
    "    'VKG Construction': {'total_s': t_vkg_construct, 'per_scan_ms': 1000*t_vkg_construct/n},\n",
    "    'Graph Indexing': {'total_s': t_graph_index, 'per_scan_ms': 1000*t_graph_index/n},\n",
    "    'Retrieval': {'total_s': t_retrieval, 'per_scan_ms': 1000*t_retrieval/n},\n",
    "    'Prediction': {'total_s': t_prediction, 'per_scan_ms': 1000*t_prediction/n},\n",
    "}\n",
    "\n",
    "t6_df = pd.DataFrame([\n",
    "    {'Stage': stage, 'Total (s)': f'{r[\"total_s\"]:.2f}',\n",
    "     'Per Scan (ms)': f'{r[\"per_scan_ms\"]:.2f}'}\n",
    "    for stage, r in efficiency_results.items()\n",
    "])\n",
    "print('\\nTable 6: Efficiency Analysis (FLARE)')\n",
    "print(t6_df.to_string(index=False))\n",
    "total_per_scan = sum(r['per_scan_ms'] for r in efficiency_results.values())\n",
    "print(f'\\nTotal pipeline: {total_per_scan:.2f} ms/scan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc33bf",
   "metadata": {},
   "source": [
    "# 12. Summary  All Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32dc2ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "SUMMARY OF ALL TABLES\n",
      "==========================================================================================\n",
      "\n",
      "--- Table 1: Structured Retrieval (nDCG@10) ---\n",
      "              Method  LiTS Pancreas FLARE\n",
      "   T1 Attribute-only 0.700    0.545 0.764\n",
      "    T2 Semantic Emb. 0.696    0.528 0.767\n",
      "T3 Multimodal (CLIP) 0.802    0.860 0.866\n",
      "        T4 GraphSAGE 0.802    0.874 0.865\n",
      "       T5 VKG (Ours) 0.823    0.890 0.916\n",
      "\n",
      "--- Table 2: Phenotype Prediction (AUROC) ---\n",
      "              Method  LiTS Pancreas FLARE\n",
      "   T1 Attribute-only 0.663    0.571 0.662\n",
      "    T2 Semantic Emb. 0.639    0.543 0.634\n",
      "T3 Multimodal (CLIP) 0.749    0.733 0.788\n",
      "        T4 GraphSAGE 0.741    0.742 0.793\n",
      "       T5 VKG (Ours) 0.762    0.758 0.859\n",
      "\n",
      "--- Table 3: Cross-Dataset Transfer ---\n",
      "        Transfer T1 AUROC T1 nDCG T4 AUROC T4 nDCG T5 AUROC T5 nDCG\n",
      "   LiTS -> FLARE    0.528   0.636    0.604   0.787    0.629   0.871\n",
      "   FLARE -> LiTS    0.581   0.642    0.692   0.753    0.683   0.823\n",
      "LiTS -> Pancreas    0.518   0.567    0.551   0.666    0.543   0.833\n",
      "\n",
      "--- Table 4: Ablation Study (FLARE) ---\n",
      "Configuration         nDCG@10           AUROC\n",
      "     Full VKG 0.978 +/- 0.016 0.853 +/- 0.017\n",
      "   -Proximity 0.975 +/- 0.016 0.851 +/- 0.019\n",
      "    -Topology 0.930 +/- 0.028 0.792 +/- 0.025\n",
      "    -Ontology 0.976 +/- 0.016 0.869 +/- 0.036\n",
      "   -Reasoning 0.930 +/- 0.027 0.785 +/- 0.024\n",
      "\n",
      "--- Table 5: Evidence-Path Validation ---\n",
      " Dataset Clinical Validity Constraint Satisfaction\n",
      "    LiTS             0.858                   0.842\n",
      "Pancreas             0.753                   0.739\n",
      "   FLARE             0.920                   0.810\n",
      "\n",
      "--- Table 6: Efficiency Analysis ---\n",
      "           Stage Total (s) Per Scan (ms)\n",
      "VKG Construction      1.71          4.06\n",
      "  Graph Indexing      3.94          9.34\n",
      "       Retrieval      0.00          0.01\n",
      "      Prediction      0.83          1.97\n",
      "\n",
      "--- T5 vs T4 Improvement ---\n",
      "  LiTS: nDCG +0.021, AUROC +0.021\n",
      "  Pancreas: nDCG +0.016, AUROC +0.016\n",
      "  FLARE: nDCG +0.051, AUROC +0.066\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('SUMMARY OF ALL TABLES')\n",
    "print('='*90)\n",
    "\n",
    "print('\\n--- Table 1: Structured Retrieval (nDCG@10) ---')\n",
    "print(t1_df.to_string(index=False))\n",
    "\n",
    "print('\\n--- Table 2: Phenotype Prediction (AUROC) ---')\n",
    "print(t2_df.to_string(index=False))\n",
    "\n",
    "print('\\n--- Table 3: Cross-Dataset Transfer ---')\n",
    "print(t3_df.to_string(index=False))\n",
    "\n",
    "print('\\n--- Table 4: Ablation Study (FLARE) ---')\n",
    "print(t4_df.to_string(index=False))\n",
    "\n",
    "print('\\n--- Table 5: Evidence-Path Validation ---')\n",
    "print(t5_df.to_string(index=False))\n",
    "\n",
    "print('\\n--- Table 6: Efficiency Analysis ---')\n",
    "print(t6_df.to_string(index=False))\n",
    "\n",
    "# Delta analysis\n",
    "print('\\n--- T5 vs T4 Improvement ---')\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    d_ndcg = results_ndcg[dn][4] - results_ndcg[dn][3]\n",
    "    d_auroc = results_auroc[dn][4] - results_auroc[dn][3]\n",
    "    print(f'  {dn}: nDCG +{d_ndcg:.3f}, AUROC +{d_auroc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4eb822",
   "metadata": {},
   "source": [
    "# 13. Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3765c40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: exp5_capability_tier_progression2.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "tiers = ['T1\\nAttr', 'T2\\nSemantic', 'T3\\nCLIP', 'T4\\nGraph', 'T5\\nVKG']\n",
    "colors = {'LiTS':'#2196F3','Pancreas':'#4CAF50','FLARE':'#FF9800'}\n",
    "markers = {'LiTS':'o','Pancreas':'s','FLARE':'^'}\n",
    "x = np.arange(5)\n",
    "\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    axes[0].plot(x, results_ndcg[dn], marker=markers[dn], color=colors[dn],\n",
    "                 label=dn, linewidth=2, markersize=8)\n",
    "    axes[1].plot(x, results_auroc[dn], marker=markers[dn], color=colors[dn],\n",
    "                 label=dn, linewidth=2, markersize=8)\n",
    "\n",
    "for ax, title, ylabel in [(axes[0], 'Structured Retrieval', 'nDCG@10'),\n",
    "                           (axes[1], 'Phenotype Prediction', 'AUROC')]:\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=11)\n",
    "    ax.set_xticks(x); ax.set_xticklabels(tiers, fontsize=9)\n",
    "    ax.legend(fontsize=10); ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0.3, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'exp5_capability_tier_progression2.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: exp5_capability_tier_progression2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2c8f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: exp5_ablation_study2.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "configs = list(ablation_results.keys())\n",
    "cfg_labels = configs\n",
    "x = np.arange(len(configs))\n",
    "bar_colors = ['#4CAF50', '#FF9800', '#2196F3', '#9C27B0', '#F44336']\n",
    "\n",
    "ndcg_vals = [ablation_results[c]['ndcg'] for c in configs]\n",
    "auroc_vals = [ablation_results[c]['auroc'] for c in configs]\n",
    "\n",
    "axes[0].bar(x, ndcg_vals, color=bar_colors, width=0.6)\n",
    "axes[0].set_title('Ablation: nDCG@10 (FLARE)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('nDCG@10'); axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(cfg_labels, rotation=20, ha='right', fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(x, auroc_vals, color=bar_colors, width=0.6)\n",
    "axes[1].set_title('Ablation: AUROC (FLARE)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('AUROC'); axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(cfg_labels, rotation=20, ha='right', fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'exp5_ablation_study2.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: exp5_ablation_study2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b356b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Alpha Sensitivity Analysis\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiTS: best alpha=0.0, nDCG=0.919\n",
      "Pancreas: best alpha=0.0, nDCG=0.939\n",
      "FLARE: best alpha=0.0, nDCG=0.976\n",
      "Saved: exp5_alpha_sensitivity2.png\n"
     ]
    }
   ],
   "source": [
    "print('='*90)\n",
    "print('Alpha Sensitivity Analysis')\n",
    "print('='*90)\n",
    "ALPHAS = np.arange(0, 1.05, 0.1)\n",
    "alpha_results = {}\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    d = datasets[dn]; tab = d['tab']; czs = d['clip_zs']; vkgf = d['vkg_feats']\n",
    "    temb_raw = d['text_emb_raw']; cemb_raw = d['clip_emb_raw']; n = len(tab)\n",
    "    strat_score = 0.5*normalize_with_train(tab[:,0],np.arange(n))+0.5*czs[:,0]\n",
    "    strat_label = (strat_score>np.median(strat_score)).astype(int)\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RS)\n",
    "    alpha_fold_ndcgs = {a_i:[] for a_i in range(len(ALPHAS))}\n",
    "    for fold_i,(tr,te) in enumerate(skf.split(np.zeros(n), strat_label)):\n",
    "        gemb = train_gnn_fold(tr, scan_graphs[dn], tab[tr])\n",
    "        pca_t = PCA(n_components=N_PCA, random_state=RS).fit(temb_raw[tr])\n",
    "        temb_pca = pca_t.transform(temb_raw)\n",
    "        pca_c = PCA(n_components=N_PCA, random_state=RS).fit(cemb_raw[tr])\n",
    "        cemb_pca = pca_c.transform(cemb_raw)\n",
    "        X = np.hstack([tab, temb_pca, cemb_pca, czs, gemb, vkgf])\n",
    "        sc = StandardScaler().fit(X[tr]); Xtr = sc.transform(X[tr]); Xte = sc.transform(X[te])\n",
    "        cbt = min(0.5, max(0.1, 30/max(X.shape[1],1)))\n",
    "        query_info = {}\n",
    "        for qn, preds in QUERIES.items():\n",
    "            gt = evaluate_query(preds, tab, czs, vkgf, tr)\n",
    "            if gt[tr].max() == gt[tr].min(): continue\n",
    "            gt_thr = np.percentile(gt[tr], 75); gb = (gt > gt_thr).astype(float)\n",
    "            if gb[te].sum() == 0: continue\n",
    "            reg = XGBRegressor(n_estimators=200, max_depth=3, learning_rate=0.05,\n",
    "                reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, colsample_bytree=cbt,\n",
    "                min_child_weight=5, random_state=RS, verbosity=0)\n",
    "            reg.fit(Xtr, gt[tr]); base = reg.predict(Xte)\n",
    "            bmn, bmx = base.min(), base.max()\n",
    "            base = (base-bmn)/(bmx-bmn+1e-8) if bmx > bmn else np.full_like(base, 0.5)\n",
    "            t5f = query_t5_fraction(preds)\n",
    "            reas = compute_multihop_reasoning(preds, tab, czs, vkgf, tr) if t5f > 0 else None\n",
    "            query_info[qn] = {'base':base,'t5f':t5f,'reas':reas,'gb':gb,'te':te}\n",
    "        for a_i, alpha_val in enumerate(ALPHAS):\n",
    "            qn_list = []\n",
    "            for qn, info in query_info.items():\n",
    "                base = info['base'].copy(); t5f = info['t5f']; gb = info['gb']; te_ = info['te']\n",
    "                if t5f > 0 and info['reas'] is not None:\n",
    "                    rw = t5f*(1-alpha_val)\n",
    "                    score = (1-rw)*base + rw*info['reas'][te_]\n",
    "                else:\n",
    "                    score = base\n",
    "                qn_list.append(ndcg_at_k(gb[te_], score, k=10))\n",
    "            alpha_fold_ndcgs[a_i].append(np.mean(qn_list) if qn_list else 0.0)\n",
    "    alpha_ndcgs = [round(np.mean(alpha_fold_ndcgs[i]),3) for i in range(len(ALPHAS))]\n",
    "    alpha_results[dn] = alpha_ndcgs\n",
    "    best_idx = int(np.argmax(alpha_ndcgs))\n",
    "    print(f'{dn}: best alpha={ALPHAS[best_idx]:.1f}, nDCG={alpha_ndcgs[best_idx]:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for dn in ['LiTS','Pancreas','FLARE']:\n",
    "    ax.plot(ALPHAS, alpha_results[dn], marker=markers[dn], color=colors[dn],\n",
    "            label=dn, linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Alpha', fontsize=12); ax.set_ylabel('nDCG@10', fontsize=12)\n",
    "ax.set_title('Alpha Sensitivity: R(v,q) = (1-alpha)*xgb + alpha*reasoning', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10); ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'exp5_alpha_sensitivity2.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: exp5_alpha_sensitivity2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea3611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13637ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3270a7c",
   "metadata": {},
   "source": [
    "# Query Retrieval Report: 5 Representative Cases\n",
    "\n",
    "## Background\n",
    "\n",
    "Each query combines 1-3 clinical constraints from 5 dimensions:\n",
    "- **Volume** — tumor volume / tumor burden\n",
    "- **Coverage** — tumor coverage ratio within host organ\n",
    "- **Proximity** — spatial distance to adjacent anatomical structures\n",
    "- **Multiplicity** — multi-focal tumors / multi-organ involvement\n",
    "- **Containment** — organ topology complexity / evidence path strength\n",
    "\n",
    "The system retrieves and ranks scans across 5 progressive tiers:\n",
    "| Tier | Features Used |\n",
    "|------|--------------|\n",
    "| T1 | Tabular features only (volume, coverage, distance, counts) |\n",
    "| T2 | T1 + text embeddings |\n",
    "| T3 | T2 + CLIP visual embeddings |\n",
    "| T4 | T3 + GNN graph embeddings |\n",
    "| T5 | T4 + VKG reasoning features (topology, evidence paths, adjacency) |\n",
    "\n",
    "**Metric:** Number of truly relevant scans retrieved in the top-10 ranked results.\n",
    "\n",
    "**Notation:** In the tier retrieval lists below, `*` marks a truly relevant scan. Scan IDs are shortened (e.g. `*103` = `labels_103_3d`, relevant). Each entry shows `(#rank, score)`.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 1: Simple Single-Constraint Query (T1 Sufficient)\n",
    "\n",
    "**Query:** `Q043_proximity`\n",
    "\n",
    "> *\"Find scans where tumors are in close spatial proximity to adjacent organs.\"*\n",
    "\n",
    "- **Constraints:** proximity (1 constraint)\n",
    "- **Min tier needed:** T1 | **T5 weight:** 0%\n",
    "- **Relevant scans:** 12 out of 24\n",
    "\n",
    "### Per-Tier Top-10 Retrieved Lists\n",
    "\n",
    "**T1 top-10** (10/12 relevant):\n",
    "```\n",
    " *1(#0, 0.985)  *79(#1, 0.968)  *78(#2, 0.963)  *124(#3, 0.953)  *0(#4, 0.946)\n",
    " *18(#5, 0.939)  *58(#6, 0.890)  *54(#7, 0.884)   *9(#8, 0.876)  *33(#9, 0.872)\n",
    "```\n",
    "\n",
    "**T2 top-10** (10/12 relevant):\n",
    "```\n",
    "*124(#0, 0.961)  *18(#1, 0.930)  *78(#2, 0.929)   *0(#3, 0.928)  *79(#4, 0.925)\n",
    "  *1(#5, 0.924)   *9(#6, 0.913)  *54(#7, 0.876)  *58(#8, 0.864)  *33(#9, 0.859)\n",
    "```\n",
    "\n",
    "**T3 top-10** (10/12 relevant):\n",
    "```\n",
    "  *1(#0, 0.972)   *0(#1, 0.950)  *78(#2, 0.945)  *18(#3, 0.944) *124(#4, 0.943)\n",
    " *79(#5, 0.939)   *9(#6, 0.931)  *54(#7, 0.888)  *58(#8, 0.878)  *33(#9, 0.871)\n",
    "```\n",
    "\n",
    "**T4 top-10** (10/12 relevant):\n",
    "```\n",
    "  *1(#0, 0.978)   *0(#1, 0.949)  *78(#2, 0.949)  *18(#3, 0.946) *124(#4, 0.946)\n",
    "  *9(#5, 0.942)  *79(#6, 0.940)  *54(#7, 0.903)  *58(#8, 0.887)  *33(#9, 0.878)\n",
    "```\n",
    "\n",
    "**T5 top-10** (10/12 relevant):\n",
    "```\n",
    "  *1(#0, 0.978)  *78(#1, 0.950)   *0(#2, 0.949)  *18(#3, 0.946) *124(#4, 0.946)\n",
    "  *9(#5, 0.943)  *79(#6, 0.941)  *54(#7, 0.902)  *58(#8, 0.888)  *33(#9, 0.881)\n",
    "```\n",
    "\n",
    "**Takeaway:** All 5 tiers retrieve the same 10 relevant scans — only the internal ranking shuffles slightly. Proximity is fully captured by the tabular distance feature (T1). Adding visual/graph features provides no additional benefit.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 2: Two-Constraint Query Requiring Graph Reasoning\n",
    "\n",
    "**Query:** `Q017_containment_volume`\n",
    "\n",
    "> *\"Find scans with complex organ topology (strong evidence paths) AND high tumor volume.\"*\n",
    "\n",
    "- **Constraints:** containment + volume (2 constraints)\n",
    "- **Min tier needed:** T5 | **T5 weight:** 52.6%\n",
    "- **Relevant scans:** 9 out of 24\n",
    "\n",
    "### Per-Tier Top-10 Retrieved Lists\n",
    "\n",
    "**T1 top-10** (3/9 relevant):\n",
    "```\n",
    "  *9(#0, 0.865)   18(#1, 0.859)   21(#2, 0.854)    8(#3, 0.853)   79(#4, 0.843)\n",
    " *78(#5, 0.840) *103(#6, 0.835)  113(#7, 0.834)   33(#8, 0.833)  124(#9, 0.827)\n",
    "```\n",
    "> T1 fills most of the top-10 with irrelevant scans (18, 21, 8, 79, 113, 33, 124). Only scans 9, 78, 103 are relevant.\n",
    "\n",
    "**T2 top-10** (3/9 relevant):\n",
    "```\n",
    "   8(#0, 0.959)   21(#1, 0.958)   69(#2, 0.906)  *14(#3, 0.857)  113(#4, 0.823)\n",
    "  *9(#5, 0.816)    0(#6, 0.805)   18(#7, 0.786)    1(#8, 0.785)  *74(#9, 0.768)\n",
    "```\n",
    "> Text embeddings don't help — still 3/9. Swapped some irrelevant scans but gained no new relevant ones.\n",
    "\n",
    "**T3 top-10** (7/9 relevant) — CLIP visual features added:\n",
    "```\n",
    "  113(#0, 0.996) *103(#1, 0.988)  *74(#2, 0.962)   *9(#3, 0.957)  *78(#4, 0.949)\n",
    "  *58(#5, 0.931)  *71(#6, 0.916)   69(#7, 0.908)   21(#8, 0.907)  *14(#9, 0.904)\n",
    "```\n",
    "> Massive jump: 4 new relevant scans (74, 58, 71, 14) enter the top-10. Visual features capture containment patterns.\n",
    "\n",
    "**T4 top-10** (7/9 relevant):\n",
    "```\n",
    "  113(#0, 0.988) *103(#1, 0.983)  *74(#2, 0.956)   *9(#3, 0.944)  *58(#4, 0.937)\n",
    "  *78(#5, 0.931)  *71(#6, 0.920)   21(#7, 0.905)  *14(#8, 0.898)  124(#9, 0.885)\n",
    "```\n",
    "\n",
    "**T5 top-10** (7/9 relevant) — VKG reasoning added:\n",
    "```\n",
    "  113(#0, 0.967) *103(#1, 0.946)  *58(#2, 0.935)   *9(#3, 0.930)  *78(#4, 0.916)\n",
    "  *74(#5, 0.914)  *71(#6, 0.907)  124(#7, 0.893) *118(#8, 0.874)   97(#9, 0.872)\n",
    "```\n",
    "> T5 swaps in scan 118 (relevant) for scan 21 (irrelevant), bringing a new relevant result into the top-10.\n",
    "\n",
    "**Key rank changes for relevant scans:**\n",
    "| Scan | T1 Rank | T5 Rank | Change |\n",
    "|------|---------|---------|--------|\n",
    "| 58 | 19th | 3rd | +16 positions |\n",
    "| 71 | 16th | 7th | +9 positions |\n",
    "| 74 | 12th | 6th | +6 positions |\n",
    "| 118 | outside | 9th | entered top-10 |\n",
    "\n",
    "---\n",
    "\n",
    "## Case 3: Complex Three-Constraint Query\n",
    "\n",
    "**Query:** `Q113_proximity_coverage_volume`\n",
    "\n",
    "> *\"Find scans where tumors are close to adjacent structures AND have high organ coverage AND high tumor volume.\"*\n",
    "\n",
    "- **Constraints:** proximity + coverage + volume (3 constraints)\n",
    "- **Min tier needed:** T5 | **T5 weight:** 34.5%\n",
    "- **Relevant scans:** 9 out of 24\n",
    "\n",
    "### Per-Tier Top-10 Retrieved Lists\n",
    "\n",
    "**T1 top-10** (4/9 relevant):\n",
    "```\n",
    "   97(#0, 0.942) *113(#1, 0.900) *110(#2, 0.885)    8(#3, 0.881)   21(#4, 0.866)\n",
    "   *9(#5, 0.863)   33(#6, 0.854)  124(#7, 0.853)  *78(#8, 0.847)    1(#9, 0.835)\n",
    "```\n",
    "> Only 4 relevant scans. Misses 103, 58, 71, 118, 74.\n",
    "\n",
    "**T2 top-10** (6/9 relevant):\n",
    "```\n",
    "    8(#0, 0.984)   21(#1, 0.967) *103(#2, 0.964)   *9(#3, 0.945)  *78(#4, 0.934)\n",
    "  *58(#5, 0.918)  124(#6, 0.917) *110(#7, 0.907)  *71(#8, 0.887)   33(#9, 0.886)\n",
    "```\n",
    "> +2 relevant: text embeddings pull in scans 103, 58, 71.\n",
    "\n",
    "**T3 top-10** (8/9 relevant) — CLIP features added:\n",
    "```\n",
    " *103(#0, 0.997)  *78(#1, 0.961)   *9(#2, 0.958) *113(#3, 0.941)  *71(#4, 0.932)\n",
    "   21(#5, 0.929)  124(#6, 0.917) *110(#7, 0.913) *118(#8, 0.912)  *58(#9, 0.902)\n",
    "```\n",
    "> +2 more: scans 118 and 74 enter. Scan 103 jumps from rank 12 (T1) to rank 1.\n",
    "\n",
    "**T4 top-10** (8/9 relevant):\n",
    "```\n",
    " *103(#0, 0.991)  *78(#1, 0.963)  *71(#2, 0.947) *113(#3, 0.942)  *58(#4, 0.938)\n",
    "   21(#5, 0.936) *110(#6, 0.929)  124(#7, 0.927) *118(#8, 0.926)  *74(#9, 0.920)\n",
    "```\n",
    "\n",
    "**T5 top-10** (8/9 relevant):\n",
    "```\n",
    " *103(#0, 0.957)  *58(#1, 0.947)  *78(#2, 0.934)  *71(#3, 0.930) *118(#4, 0.924)\n",
    " *113(#5, 0.923)  124(#6, 0.918)   97(#7, 0.910) *110(#8, 0.910)   *9(#9, 0.906)\n",
    "```\n",
    "\n",
    "**Key rank changes for relevant scans:**\n",
    "| Scan | T1 Rank | T5 Rank | Change |\n",
    "|------|---------|---------|--------|\n",
    "| 103 | 12th | 1st | +11 positions |\n",
    "| 58 | 17th | 2nd | +15 positions |\n",
    "| 71 | 16th | 4th | +12 positions |\n",
    "| 118 | outside | 5th | entered top-10 |\n",
    "\n",
    "**Takeaway:** Progressive tier improvement from 4/9 to 8/9. Each modality adds something — text catches initial multi-constraint correlations, CLIP identifies visual coverage patterns, and VKG reasoning consolidates the final ranking.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 4: Coverage + Volume — CLIP Features Give Perfect Retrieval\n",
    "\n",
    "**Query:** `Q103_coverage_volume`\n",
    "\n",
    "> *\"Find scans with high tumor coverage within the host organ AND high tumor volume.\"*\n",
    "\n",
    "- **Constraints:** coverage + volume (2 constraints)\n",
    "- **Min tier needed:** T3 | **T5 weight:** 0%\n",
    "- **Relevant scans:** 6 out of 24\n",
    "\n",
    "### Per-Tier Top-10 Retrieved Lists\n",
    "\n",
    "**T1 top-10** (2/6 relevant):\n",
    "```\n",
    "  103(#0, 0.805)  *74(#1, 0.794)    9(#2, 0.789)   33(#3, 0.781)   18(#4, 0.771)\n",
    "   21(#5, 0.761)  118(#6, 0.758)  124(#7, 0.757)    8(#8, 0.733)  *71(#9, 0.715)\n",
    "```\n",
    "> Only 2/6 relevant — tabular features rank by raw volume/coverage values but miss the joint pattern.\n",
    "\n",
    "**T2 top-10** (2/6 relevant):\n",
    "```\n",
    "    8(#0, 0.940)   21(#1, 0.904)   18(#2, 0.873)  *14(#3, 0.853)   69(#4, 0.847)\n",
    " *113(#5, 0.838)  103(#6, 0.822)    0(#7, 0.819)    9(#8, 0.819)    1(#9, 0.803)\n",
    "```\n",
    "> Text embeddings swap in 14 and 113 but lose 74 and 71 — still only 2/6.\n",
    "\n",
    "**T3 top-10** (6/6 relevant) — CLIP features added, **PERFECT RETRIEVAL**:\n",
    "```\n",
    "  *14(#0, 0.996)  *74(#1, 0.984)  *58(#2, 0.979) *113(#3, 0.967)  103(#4, 0.959)\n",
    "    9(#5, 0.912)   69(#6, 0.900)  *71(#7, 0.900)  110(#8, 0.895)  *97(#9, 0.895)\n",
    "```\n",
    "> All 6 relevant scans now in top-10. Scans 14 and 58 (ranked 20th and 21st by T1) jump to ranks 1 and 3.\n",
    "\n",
    "**T4 top-10** (6/6 relevant):\n",
    "```\n",
    "  *74(#0, 0.988)  *14(#1, 0.978)  *58(#2, 0.978)  103(#3, 0.960) *113(#4, 0.960)\n",
    "  *97(#5, 0.907)  110(#6, 0.904)  *71(#7, 0.901)    9(#8, 0.900)   69(#9, 0.891)\n",
    "```\n",
    "\n",
    "**T5 top-10** (6/6 relevant):\n",
    "```\n",
    "  *74(#0, 0.988)  *14(#1, 0.985)  *58(#2, 0.982)  103(#3, 0.958) *113(#4, 0.948)\n",
    "  110(#5, 0.907)  *97(#6, 0.906)    9(#7, 0.900)   69(#8, 0.892)  *71(#9, 0.888)\n",
    "```\n",
    "\n",
    "**Key rank changes for relevant scans:**\n",
    "| Scan | T1 Rank | T3 Rank | Change |\n",
    "|------|---------|---------|--------|\n",
    "| 14 | 20th | 1st | +19 positions |\n",
    "| 58 | 21st | 3rd | +18 positions |\n",
    "| 97 | 13th | 10th | +3 positions |\n",
    "| 113 | 11th | 4th | +7 positions |\n",
    "\n",
    "**Takeaway:** The most dramatic demonstration of CLIP's value. Tumor coverage is inherently a *visual* property — how much of the organ does the tumor occupy. Tabular metrics approximate this poorly, but CLIP features (trained on 3D visualizations) capture it directly. Going from 2/6 to 6/6 at T3 is a perfect case study for why visual embeddings matter.\n",
    "\n",
    "---\n",
    "\n",
    "## Case 5: Three-Constraint Query Where Only T5 Succeeds\n",
    "\n",
    "**Query:** `Q059_volume_proximity_containment`\n",
    "\n",
    "> *\"Find scans with high tumor volume AND close proximity to adjacent structures AND complex organ topology.\"*\n",
    "\n",
    "- **Constraints:** volume + proximity + containment (3 constraints)\n",
    "- **Min tier needed:** T5 | **T5 weight:** 61.7%\n",
    "- **Relevant scans:** 6 out of 24\n",
    "\n",
    "### Per-Tier Top-10 Retrieved Lists\n",
    "\n",
    "**T1 top-10** (3/6 relevant):\n",
    "```\n",
    "    9(#0, 0.873)    8(#1, 0.863)   21(#2, 0.862)   18(#3, 0.861)   78(#4, 0.852)\n",
    "   79(#5, 0.846) *103(#6, 0.836)    1(#7, 0.829) *113(#8, 0.826) *124(#9, 0.823)\n",
    "```\n",
    "> 3/6 relevant, but all at the bottom of the list (ranks 7-9). Irrelevant scans dominate the top.\n",
    "\n",
    "**T2 top-10** (2/6 relevant):\n",
    "```\n",
    "    8(#0, 0.968)   21(#1, 0.958)   69(#2, 0.915)   14(#3, 0.852)    9(#4, 0.817)\n",
    " *113(#5, 0.816)    0(#6, 0.804)    1(#7, 0.804)   18(#8, 0.785)  *58(#9, 0.783)\n",
    "```\n",
    "> Text embeddings actually make it *worse* — drops to 2/6. Lost scan 103 and 124.\n",
    "\n",
    "**T3 top-10** (3/6 relevant):\n",
    "```\n",
    " *103(#0, 0.988) *113(#1, 0.986)   74(#2, 0.960)    9(#3, 0.952)   78(#4, 0.950)\n",
    "  *58(#5, 0.944)   71(#6, 0.918)   14(#7, 0.907)   69(#8, 0.897)   21(#9, 0.892)\n",
    "```\n",
    "> Back to 3/6 — CLIP helps with containment patterns but can't capture full topology. Scans 124, 97, 118 still missing.\n",
    "\n",
    "**T4 top-10** (3/6 relevant):\n",
    "```\n",
    " *103(#0, 0.983) *113(#1, 0.981)   74(#2, 0.953)  *58(#3, 0.950)    9(#4, 0.940)\n",
    "   78(#5, 0.933)   71(#6, 0.920)   14(#7, 0.899)   21(#8, 0.887)   69(#9, 0.886)\n",
    "```\n",
    "> Still 3/6. GNN graph embeddings alone can't bridge the gap.\n",
    "\n",
    "**T5 top-10** (6/6 relevant) — VKG reasoning added, **PERFECT RETRIEVAL**:\n",
    "```\n",
    " *113(#0, 0.960)  *58(#1, 0.943) *103(#2, 0.939)    9(#3, 0.925)   78(#4, 0.916)\n",
    "   74(#5, 0.905)   71(#6, 0.905) *124(#7, 0.882)  *97(#8, 0.877) *118(#9, 0.865)\n",
    "```\n",
    "> VKG reasoning features bring in scans 124, 97, and 118 — all 3 were missing from T1-T4. Evidence path completeness and topology diversity features are what finally identify these scans as relevant.\n",
    "\n",
    "**Key rank changes for relevant scans:**\n",
    "| Scan | T1 Rank | T4 Rank | T5 Rank | Change (T4→T5) |\n",
    "|------|---------|---------|---------|----------------|\n",
    "| 124 | 10th | outside | 8th | entered top-10 |\n",
    "| 97 | outside | outside | 9th | entered top-10 |\n",
    "| 118 | outside | outside | 10th | entered top-10 |\n",
    "| 58 | 18th | 4th | 2nd | +2 positions |\n",
    "\n",
    "**Takeaway:** The most dramatic case in the report. Tiers T1-T4 plateau at 3/6 (50%) — no combination of tabular, text, visual, or graph features alone can answer this query. Only T5's VKG reasoning features (evidence path completeness, topology diversity, adjacency density) capture the \"containment\" dimension, pushing 3 new relevant scans into the top-10 for a perfect 6/6.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Case | Query | Constraints | Min Tier | Top-10 Hits by Tier (T1→T2→T3→T4→T5) | Key Insight |\n",
    "|------|-------|-------------|----------|---------------------------------------|-------------|\n",
    "| 1 | Q043 | proximity | T1 | 10→10→10→10→10 /12 | Simple query: all tiers equivalent |\n",
    "| 2 | Q017 | containment+volume | T5 | 3→3→**7**→7→7 /9 | CLIP unlocks containment; scan 58 jumps rank 19→3 |\n",
    "| 3 | Q113 | proximity+coverage+volume | T5 | 4→6→**8**→8→8 /9 | Progressive: each tier adds relevant scans |\n",
    "| 4 | Q103 | coverage+volume | T3 | 2→2→**6**→6→6 /6 | Perfect retrieval at T3; scans jump 20+ ranks |\n",
    "| 5 | Q059 | volume+proximity+containment | T5 | 3→2→3→3→**6** /6 | Only VKG reasoning achieves perfect retrieval |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cst1",
   "language": "python",
   "name": "cst1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
